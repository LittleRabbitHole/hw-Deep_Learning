{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295593 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "wrhta qcdal mnib tnorh ca gvijoaikmrl ir mtqdmacj  eshieefbyv  zhaizerjsequlnsek\n",
      "msxiaeddpaen sds yxw btfzorarwwy r qgpl tfdfr elazij fyhsaqkboxu sravp i  kvrmhg\n",
      "pdqfakmeihwsudeqwmip ejdri uehihe lijtpboqivbasxmflab aeovmisp  gxkdkfzg evbovc \n",
      "rp hjufnazocouxeap yggkvuahiezvj  tjjpawckqgzi ttfemt rfudqimle up e vrsnp hln m\n",
      "ou qsixduagwv j uflrcs  lvi  a d utrx agjfdl iza sswjthj  i  br lgfv jlpxskihhmm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.08\n",
      "Average loss at step 100: 2.594799 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.88\n",
      "Validation set perplexity: 10.19\n",
      "Average loss at step 200: 2.249277 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.56\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 300: 2.105737 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 400: 2.011673 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 500: 1.944296 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 600: 1.916360 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 700: 1.868751 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800: 1.825730 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 900: 1.837311 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1000: 1.833367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "gen sidsteving the veram predistion wrule excometion makhias froviviate desiglat\n",
      "pen carle kurden to dieeralally deveter ove firm a knoudh usees rammer disted kn\n",
      "che mond expedetion of a decimation becing isto beage and chare bes evurmely mad\n",
      "gide dicitar scombleation as befare gareveria tely whoco word and undic usee lep\n",
      "g colte mesime operitidiof ofrectly lefferera recobricalugar and comple asseats \n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1100: 1.785222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1200: 1.761779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.743267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1400: 1.754418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1500: 1.743500 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1600: 1.755001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1700: 1.717625 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.680813 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1900: 1.654875 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2000: 1.703451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "h pysing to wave tiberent theinge darkand the s of the comporodus fren was simpl\n",
      "miniin and inmids first life latist of howes not and in usting named diring had \n",
      "s groung wames verences maguited cloon quen single it is bound it unjodity six d\n",
      "quatiquent quaprientary dapjand northings churfures and linched te the hos bandy\n",
      "ogin grissiall ari othir g ages and four the is regiles indgurs onities in to th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.694179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2200: 1.684607 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2300: 1.645856 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2400: 1.667643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2500: 1.687165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.662122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2700: 1.666711 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2800: 1.654373 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2900: 1.659409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3000: 1.658340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "flerized encerved corday bm the than fiderowment s retwole have recarleetes and \n",
      "ver di form of authen lewricalled pplations evect externally successical senqure\n",
      "reng is cords evante canls it his soul resuorass its  s groxs which war mp e sta\n",
      "bord one nine own the argenicalishes opended whiching in the played eight kalio \n",
      "ven industrie compover of a yownical coti when ocour accodser of groveral catern\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3100: 1.636367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.654166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3300: 1.646384 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3400: 1.679231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3500: 1.663733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3600: 1.674889 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3700: 1.653448 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3800: 1.650981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3900: 1.640952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4000: 1.657217 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "mental his preclarg and the number repuople the predious pitry merts by the prov\n",
      "nosed for centrains all hohdes of fasches croou or freet bit her exting theory i\n",
      "x albamine to pepppor ko vary practice in commgnity kenory and a coins shopuld b\n",
      "quentry mayse persker alpeares these and that broined theorvage games most the s\n",
      "um mirusasion six six zero four a cred aweminar german medectal charas orsginall\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4100: 1.636465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4200: 1.641133 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4300: 1.620132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4400: 1.612997 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4500: 1.619884 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4600: 1.617539 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4700: 1.634839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4800: 1.631066 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4900: 1.640382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.614747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "e drand related to to power conderiatholaws hod and four zero zero zero allac sh\n",
      "zet of frectan dave trade canon intiolvy lept this edalsh by a granga resuction \n",
      "quen and house and as king goez effective a musection mother europle who mentaso\n",
      "en american fatored with though move subcoded disillia six insidu mile not actua\n",
      "jod hester papetid i is he user two zero zero zero six shialing basely othes for\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5100: 1.609877 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5200: 1.597681 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5300: 1.584348 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5400: 1.582203 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.567828 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5600: 1.585476 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700: 1.572729 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.582910 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.578425 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.550244 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "nam zan similar official dease perse aleory archetshel the the croy sattlin in f\n",
      "ne lakerallg diam theory arar fonting impiet specifie countiaf of nolulid spreas\n",
      "ned s out the sainekle lyannen six six of brian  is empition is prodeer uson bui\n",
      "inst way a politch mailer bluser canigy tran vertes end homes rexadain mardina t\n",
      "jurings states on the fake to tower centrances detarte ordersus in place tend of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.566213 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6200: 1.543456 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.546170 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6400: 1.543747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6500: 1.555722 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6600: 1.597092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.578524 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6800: 1.606386 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.585901 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 7000: 1.577720 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "vie such as epotes in thought non a been contracted of major enencens of becomes\n",
      "noinceed wigh greathority tim is majin recectured by exolpres the lablied had ta\n",
      "m other companing to charget pody was plauts laurbed a to pundono pexantians wia\n",
      "ly of notes sciencism mach as subscels a dasses enter sixnobilys in a garalaiber\n",
      "rigariant track imagige on impleas of many article mation cavile theorkeriture t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # combined matrix\n",
    "  xx = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  mm = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bb = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    #state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    y = tf.matmul(i, xx) + tf.matmul(o, mm) + bb\n",
    "    input_gate = tf.sigmoid(y[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(y[:,num_nodes:(2*num_nodes)])\n",
    "    update = tf.tanh(y[:,(2*num_nodes):(3*num_nodes)])\n",
    "    output_gate = tf.sigmoid(y[:,(3*num_nodes):])\n",
    "    state = forget_gate * state + input_gate * update\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299434 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.10\n",
      "================================================================================\n",
      "x k  ouhnsudwpgenrlcrga f ul rpenhvgghctvkakmmogzeifeiw  s  zehav ik pioh i awwi\n",
      "wc e arz kqscdlsyyucqojihmnlnbqrojit  azgnegalkrstho  eed  q srwrsvpa aiayyio ul\n",
      "mpo p    arhz ltanu oitiazkh hganzybe c hs snsa vm hz tvssmckneddy ddtk nr fkr l\n",
      "ggiveb reew  tnhoiosd nhtqsrawhw  dc  qor guevjinyeopkotsnufwoyshoeegseds tr hm \n",
      "q vtoyyrkhe p iaqlhfuzib kv w tesyrhaqmaaiphwfha utc lu eushegtvo aei zo u  mkpg\n",
      "================================================================================\n",
      "Validation set perplexity: 19.84\n",
      "Average loss at step 100: 2.584968 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.01\n",
      "Validation set perplexity: 10.75\n",
      "Average loss at step 200: 2.251238 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 8.96\n",
      "Average loss at step 300: 2.085116 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 400: 1.999481 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 500: 1.997509 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 600: 1.922584 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 700: 1.893693 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 800: 1.875064 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 900: 1.859898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.790831 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "================================================================================\n",
      "winiti bury to sendirution one nine five seve nine three f fis and med dare rese\n",
      "c retermented a tress sqmedched bake diagted peckincly fators orcestueclo fos in\n",
      "vision and of the plipcect masty passian nucored to deperida im use which the re\n",
      "pen intoading it edcayed in a secon b inormentic aters or ga of d in one nine si\n",
      "s tensician anc dy hulook of lecictionalasions ix wlider of lijee peopuer is fre\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100: 1.768636 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1200: 1.789880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1300: 1.772496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1400: 1.746023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500: 1.735357 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1600: 1.721418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1700: 1.741119 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1800: 1.707265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.712851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2000: 1.723038 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "================================================================================\n",
      "th crounck desid five anshory conectur scarsistifikian ablward who other ird gra\n",
      "x hame a fadionided in the unceogs mowntard kinshire is othistore the even histe\n",
      "perescork a compal a brestral will sele treed tere first orcam habouter the dell\n",
      "lag over it intervation hather raylly to ds of deleghont a phisming seevally beg\n",
      " alkobgew lite a souges saltenaiards factory of filliczay veoger way the defic t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2100: 1.706174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2200: 1.676993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2300: 1.688902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2400: 1.691865 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2500: 1.709546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2600: 1.680496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2700: 1.695568 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2800: 1.655420 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2900: 1.663529 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3000: 1.665544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "================================================================================\n",
      "ther a adate or the aurary of a defered lire an is not is opp ommand gruns prexe\n",
      "que cark religion as orcaser nivery che sellajeral three divebence three six zer\n",
      "jolors broughtorit betoblegia many zereu dreatwtensers outment rist semo ramarit\n",
      "duces rocfi the evilougbitapor to evilopians and his fros their defores and rirg\n",
      "reitratol dachade to the cearwin schourt mightard of his the treets notit articu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3100: 1.661032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3200: 1.653002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3300: 1.641154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3400: 1.640977 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3500: 1.633718 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3600: 1.637217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3700: 1.639428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3800: 1.629327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3900: 1.623678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4000: 1.627490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "y inhorder duofly cluitinal for pure new his icmessalled ruza the burger secone \n",
      "x accups greats of the uniting hand two eight five nine two zero z milinant wear\n",
      "zer thirhonedally brow to nalishor occulating the commary for from e withime aga\n",
      "jocy cantstile faclorahdranted upultmestancial soutual curres of keegy the ugher\n",
      "ta kaild the sqnet atcexio relusials as foomuallels that common ablle hour statt\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4100: 1.631299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4200: 1.618585 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4300: 1.594197 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4400: 1.623666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4500: 1.633614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4600: 1.633019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.606275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4800: 1.591461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4900: 1.602949 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5000: 1.627574 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "waving president as brear stitting equired from lines lincolvorable with and int\n",
      "gonimides incluary the spectan demore keem nt two minish l kands siquali ciccutr\n",
      "y cares muatagua lines cons bestan afrog create eny towan the of respontial spec\n",
      "quarl finct gioptions and assians protest engorytral ma in sout the herguel fego\n",
      "y in the appeaaly presist unentains knobal fince the he two seven comobes apart \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5100: 1.639185 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5200: 1.632599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5300: 1.597080 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5400: 1.595786 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5500: 1.590300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5600: 1.614538 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5700: 1.574000 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5800: 1.575203 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5900: 1.594296 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6000: 1.561301 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "tok in shostal sojouse clopleciage enmituretach risk contenced liques in be well\n",
      "jal one nine nine nine ungt abreing milers a couble years in alvariath normaptar\n",
      "gene paollem be consoctote no the seven eight autonition of a knemaly in is bell\n",
      "gove place deque interpretupes uf a combunific on a c evionate influence include\n",
      "zems d was ale after imsirned and cathercal bukgips by sue alto upier ordersion \n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6100: 1.579855 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6200: 1.597925 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6300: 1.607935 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6400: 1.640737 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6500: 1.638288 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6600: 1.607931 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6700: 1.597442 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6800: 1.576653 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6900: 1.567949 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7000: 1.579232 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "urzs processive they carlandd feature them squdfully horotass about the nation a\n",
      "y attranal an acceaced loveter of world popula first can the ritherws a somel a \n",
      "hrand present defomition a lecaft adms stration heders can induce and freqority \n",
      "vistics states what the pard his duch paily ns resultation and stitle when cass \n",
      "k indivilities of poltain dhylla gen by the gene marburz which in the swented za\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part generates the embedding using the ordered neighbor to predict the bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess done!\n"
     ]
    }
   ],
   "source": [
    "# generate embedding for bigram\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# prepare data\n",
    "def bigram_encoder(s):\n",
    "    return char2id(s[0]) * 27 + char2id(s[1])\n",
    "\n",
    "def bigram_decoder(n):\n",
    "    return id2char(n//27) + id2char(n%27)\n",
    "\n",
    "bigram_size = len(text) // 2\n",
    "data = [bigram_encoder(text[(2*i):(2*i+2)]) for i in xrange(bigram_size)]\n",
    "print('Preprocess done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te']\n",
      "batch: [[' a', 'rc'], ['na', 'hi'], ['rc', 'sm'], ['hi', ' o'], ['sm', 'ri'], [' o', 'gi'], ['ri', 'na'], ['gi', 'te']]\n",
      "labels: ['na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na']\n"
     ]
    }
   ],
   "source": [
    "# generate training batches\n",
    "data_index = 0\n",
    "def generate_batch(batch_size, nb_window):\n",
    "    global data_index\n",
    "    span = (nb_window * 2 + 1)\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    batch = np.ndarray(shape=(batch_size, 2*nb_window), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    for _ in xrange(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % bigram_size\n",
    "    for i in xrange(batch_size):\n",
    "        labels[i] = buffer[nb_window]\n",
    "        for j in xrange(nb_window):\n",
    "            batch[i, j] = buffer[j]\n",
    "        for j in xrange(nb_window):\n",
    "            batch[i, (j + nb_window)] = buffer[nb_window + 1 + j]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % bigram_size\n",
    "    return batch, labels\n",
    "                      \n",
    "print('data:', [bigram_decoder(di) for di in data[:10]])\n",
    "data_index = 0\n",
    "batch, labels = generate_batch(8, 1)\n",
    "print('batch:', [[bigram_decoder(bj) for bj in bi] for bi in batch])\n",
    "print('labels:', [bigram_decoder(bi) for bi in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def embeding calc\n",
    "import math\n",
    "\n",
    "vocabulary_size = 27 ** 2\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "nb_window = 3\n",
    "valid_size = 8\n",
    "valid_samples = np.array(random.sample(range(vocabulary_size), valid_size))\n",
    "num_sampled = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    train_dataset = tf.placeholder(tf.int32, shape = [batch_size, 2*nb_window])\n",
    "    train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_samples, dtype=tf.int32)\n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size,\n",
    "                                                       embedding_size], stddev=1.0/math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    embed = tf.reduce_mean(embed, 1) # use average of neighbors\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "        weights = softmax_weights, biases = softmax_biases, labels = train_labels, inputs = embed,\n",
    "        num_sampled = num_sampled, num_classes = vocabulary_size))\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init done\n",
      "Average loss after step 0 is 4.077875\n",
      "Nearest to vh:\n",
      "['dn', 'gg', 'vo', 'af', 'jq', 'az', ' y', 'ci']\n",
      "Nearest to mm:\n",
      "['oy', 'es', 'lz', 'st', 'hl', 'z ', 'hy', 'jx']\n",
      "Nearest to ha:\n",
      "['rx', 'pj', 'we', 'wc', 'rn', 'en', 're', 've']\n",
      "Nearest to ge:\n",
      "['hy', 'tw', 'mv', 'ut', ' f', 'cj', 'np', 'zg']\n",
      "Nearest to zz:\n",
      "['fb', 'kg', 'od', 'sx', 'cf', 'sg', 'dm', 'ru']\n",
      "Nearest to co:\n",
      "['ql', 'pi', 'jy', 'no', 'dw', 'rm', 'de', 'te']\n",
      "Nearest to r :\n",
      "['wz', 'jx', 'm ', 'rf', 'vu', 'tb', 'gc', 'mz']\n",
      "Nearest to lf:\n",
      "['cp', 'ek', 'ef', 'mv', 'vv', 'xq', 'ty', 'eq']\n",
      "Average loss after step 2000 is 2.222219\n",
      "Average loss after step 4000 is 1.909565\n",
      "Average loss after step 6000 is 1.864819\n",
      "Average loss after step 8000 is 1.805884\n",
      "Average loss after step 10000 is 1.764645\n",
      "Nearest to vh:\n",
      "['ty', 'ci', 'na', 'ap', 'vo', 'dn', 'az', 'jq']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'nf', 'lo', 'ns', 'oy', 'mb', 'st']\n",
      "Nearest to ha:\n",
      "['wo', 'he', 'wa', 'bu', 'hu', 'yv', 'fi', 'ru']\n",
      "Nearest to ge:\n",
      "['ix', 'ce', 'te', 'mv', 'np', 'nk', 'zj', 'ls']\n",
      "Nearest to zz:\n",
      "['fb', 'kg', 'ld', 'od', 'ff', 'ua', 'sx', ' v']\n",
      "Nearest to co:\n",
      "[' i', 'ql', 'dw', 'ca', 'de', 'gm', 'cv', 'pr']\n",
      "Nearest to r :\n",
      "['f ', 'm ', 'rd', 'n ', 'rc', ' z', 'd ', 'rm']\n",
      "Nearest to lf:\n",
      "['ty', 'ay', 'ef', 'ev', 'ls', 'cc', 'eq', 'yf']\n",
      "Average loss after step 12000 is 1.751544\n",
      "Average loss after step 14000 is 1.777993\n",
      "Average loss after step 16000 is 1.767521\n",
      "Average loss after step 18000 is 1.716079\n",
      "Average loss after step 20000 is 1.749651\n",
      "Nearest to vh:\n",
      "['ty', 'vo', 'ci', 'na', 'ap', 'wl', 'az', 'ym']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'nf', 'mb', 'rp', 'js', 'ni', 'st']\n",
      "Nearest to ha:\n",
      "['hu', 'wo', 'wa', 'bu', 'fi', 'he', 'ru', 'oe']\n",
      "Nearest to ge:\n",
      "['ix', 'mv', 'np', 'nk', 'ye', 'jo', 'ki', 'gi']\n",
      "Nearest to zz:\n",
      "['ld', 'fb', 'kg', 'nn', 'ua', 'ff', 'od', 'na']\n",
      "Nearest to co:\n",
      "[' i', 'sy', 'pr', 'pv', 'ql', 'ze', 'cj', 'cv']\n",
      "Nearest to r :\n",
      "['f ', 'm ', 'd ', 'rf', 'rs', 'rc', 'c ', 'p ']\n",
      "Nearest to lf:\n",
      "['au', 'cc', 'ls', 'ms', 'ev', 'ty', 'av', 'ct']\n",
      "Average loss after step 22000 is 1.747849\n",
      "Average loss after step 24000 is 1.737372\n",
      "Average loss after step 26000 is 1.705396\n",
      "Average loss after step 28000 is 1.676815\n",
      "Average loss after step 30000 is 1.644527\n",
      "Nearest to vh:\n",
      "['ty', 'ci', 'vo', 'na', 'wl', 'up', 'az', 'ym']\n",
      "Nearest to mm:\n",
      "['mp', 'nf', 'nv', 'rp', 'pt', 'mb', 'gn', 'js']\n",
      "Nearest to ha:\n",
      "['wo', 'hu', 'bu', 'he', 'hr', 'hi', 'ru', 'wa']\n",
      "Nearest to ge:\n",
      "['ye', 'ix', 'fr', 'ht', 'np', 'mv', 'ki', 'ke']\n",
      "Nearest to zz:\n",
      "['ld', 'kg', 'fb', 'nn', 'na', 'ff', 'rk', 'ht']\n",
      "Nearest to co:\n",
      "['pr', 'sy', ' i', 'fo', 'ru', 'cv', 'wh', 'j ']\n",
      "Nearest to r :\n",
      "['f ', 'd ', 'rs', 'm ', 'rc', 'tb', 'rf', 'rm']\n",
      "Nearest to lf:\n",
      "['au', 'ls', 'ty', 'cc', 'rv', 'ms', 'ze', 'eq']\n",
      "Average loss after step 32000 is 1.666593\n",
      "Average loss after step 34000 is 1.727344\n",
      "Average loss after step 36000 is 1.686591\n",
      "Average loss after step 38000 is 1.598417\n",
      "Average loss after step 40000 is 1.698957\n",
      "Nearest to vh:\n",
      "['ty', 'vo', 'ci', 'up', 'na', 'ap', 'wl', 'az']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'rp', 'nf', 'mb', 'gn', 'ph', 'nn']\n",
      "Nearest to ha:\n",
      "['wo', 'hu', 'bu', 'hi', 'wa', 'fi', 'he', 'hr']\n",
      "Nearest to ge:\n",
      "['ye', 'jo', 'np', 'mv', 'fr', 'ix', 'ke', 'ht']\n",
      "Nearest to zz:\n",
      "['ld', 'kg', 'fb', 'rk', 'nn', 'ff', 'wr', 'ua']\n",
      "Nearest to co:\n",
      "['ai', 'sy', 'cv', 'we', 'sa', 'so', 'pr', 'ru']\n",
      "Nearest to r :\n",
      "['f ', 'rs', 'd ', 'rf', 'tb', 'm ', 'rc', 'l ']\n",
      "Nearest to lf:\n",
      "['au', 'rv', 'ls', 'ty', 'ze', 'pp', 'eq', 'ms']\n",
      "Average loss after step 42000 is 1.694122\n",
      "Average loss after step 44000 is 1.646960\n",
      "Average loss after step 46000 is 1.700063\n",
      "Average loss after step 48000 is 1.695286\n",
      "Average loss after step 50000 is 1.721552\n",
      "Nearest to vh:\n",
      "['ty', 'vo', 'ci', 'up', 'na', 'wl', 'ym', 'az']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'rp', 'nf', 'mb', 'ph', 'gn', 'nn']\n",
      "Nearest to ha:\n",
      "['wo', 'hu', 'bu', 'hi', 'fi', 'he', 'hr', 'ao']\n",
      "Nearest to ge:\n",
      "['ye', 'oi', 'np', 'jo', 'uh', 'gi', 'ki', 'eg']\n",
      "Nearest to zz:\n",
      "['ld', 'kg', 'fb', 'nn', 'rk', 'ff', 'ua', 'tl']\n",
      "Nearest to co:\n",
      "['sy', 'we', 'fo', ' i', 'ru', 'fe', 'cv', 'fl']\n",
      "Nearest to r :\n",
      "['f ', 'rs', 'd ', 'tb', 'm ', 'rw', 'c ', 'rm']\n",
      "Nearest to lf:\n",
      "['rv', 'au', 'ls', 'ze', 've', 'bs', 'rk', 'pp']\n",
      "Average loss after step 52000 is 1.684685\n",
      "Average loss after step 54000 is 1.676561\n",
      "Average loss after step 56000 is 1.695466\n",
      "Average loss after step 58000 is 1.694673\n",
      "Average loss after step 60000 is 1.573760\n",
      "Nearest to vh:\n",
      "['vo', 'ty', 'up', 'na', 'wl', 'ci', 'ym', 'ue']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'nf', 'rp', 'gn', 'mb', 'ph', 'eb']\n",
      "Nearest to ha:\n",
      "['wo', 'bu', 'hu', 'he', 'fi', 'hi', 'nu', 'bj']\n",
      "Nearest to ge:\n",
      "['ye', 'jo', 'oi', 'mv', 'np', 'ws', 'my', 'ik']\n",
      "Nearest to zz:\n",
      "['kg', 'ld', 'rk', 'fb', 'tl', 'nn', 'cs', 'nu']\n",
      "Nearest to co:\n",
      "['sy', 'so', 'eq', 'fl', 'pr', 'ga', 'cv', 'fo']\n",
      "Nearest to r :\n",
      "['f ', 'tb', 'rs', 'rw', 'w ', 'm ', 'd ', ' t']\n",
      "Nearest to lf:\n",
      "['au', 'rv', 'ls', 'rk', 'ze', 'eq', 'wr', 'pp']\n",
      "Average loss after step 62000 is 1.593029\n",
      "Average loss after step 64000 is 1.576971\n",
      "Average loss after step 66000 is 1.641448\n",
      "Average loss after step 68000 is 1.662885\n",
      "Average loss after step 70000 is 1.653441\n",
      "Nearest to vh:\n",
      "['vo', 'up', 'ty', 'ci', 'wl', 'ym', 'na', 'az']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'rp', 'nf', 'rv', 'mb', 'gn', 'eb']\n",
      "Nearest to ha:\n",
      "['wo', 'hu', 'hi', 'he', 'hr', 'bu', 'ww', 'oa']\n",
      "Nearest to ge:\n",
      "['jo', 'yi', 'oi', 'ye', 'fr', 'np', 'mv', 'ny']\n",
      "Nearest to zz:\n",
      "['kg', 'ld', 'nn', 'fb', 'gn', 'tl', 'cs', 'rk']\n",
      "Nearest to co:\n",
      "['pr', 'fl', ' i', 'sy', 'eq', 'ba', 'ga', 'ex']\n",
      "Nearest to r :\n",
      "['f ', 'd ', 'rs', 'rw', 'rm', 'c ', 'tb', 'n ']\n",
      "Nearest to lf:\n",
      "['rv', 'au', 'ze', 'rk', 've', 'ls', 'rl', 'dd']\n",
      "Average loss after step 72000 is 1.662408\n",
      "Average loss after step 74000 is 1.687796\n",
      "Average loss after step 76000 is 1.657514\n",
      "Average loss after step 78000 is 1.686385\n",
      "Average loss after step 80000 is 1.656243\n",
      "Nearest to vh:\n",
      "['vo', 'ty', 'up', 'ci', 'wl', 'na', 'ym', 'rk']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'rp', 'nf', 'rv', 'eb', 'pt', 'rd']\n",
      "Nearest to ha:\n",
      "['wo', 'hu', 'bu', 'hi', 'he', 'wa', 'fi', 'ww']\n",
      "Nearest to ge:\n",
      "['ye', 'oi', 'yi', 'np', 'sq', 'ja', 'ht', 'mv']\n",
      "Nearest to zz:\n",
      "['ld', 'kg', 'nn', 'fb', 'tl', 'dg', 'gn', 'ua']\n",
      "Nearest to co:\n",
      "['pr', 'fo', 'fl', 'ze', 'fe', 'fa', 'sy', 'fu']\n",
      "Nearest to r :\n",
      "['f ', 'm ', 'd ', 'rw', 'rm', 'tb', 'rs', 'c ']\n",
      "Nearest to lf:\n",
      "['rv', 'ze', 'au', 'dd', 'fg', 'rk', 'pp', 've']\n",
      "Average loss after step 82000 is 1.672925\n",
      "Average loss after step 84000 is 1.659264\n",
      "Average loss after step 86000 is 1.548595\n",
      "Average loss after step 88000 is 1.682711\n",
      "Average loss after step 90000 is 1.668722\n",
      "Nearest to vh:\n",
      "['vo', 'ty', 'up', 'na', 'wl', 'ci', 'ym', 'rk']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'nf', 'eb', 'rp', 'rv', 'pp', 'rd']\n",
      "Nearest to ha:\n",
      "['wo', 'hu', 'hi', 'bu', 'wa', 'ww', 'he', 'oa']\n",
      "Nearest to ge:\n",
      "['ye', 'oi', 'jo', 'eg', 'fr', 'yi', 'ja', 'iq']\n",
      "Nearest to zz:\n",
      "['nn', 'dg', 'tl', 'kg', 'fb', 'ld', 'gn', 'nu']\n",
      "Nearest to co:\n",
      "['fl', 'fo', 'ba', 'ze', 'fa', 'ga', 'pr', 'eq']\n",
      "Nearest to r :\n",
      "['f ', 'rw', 'd ', 'tb', 'rs', 'w ', 'm ', 'pm']\n",
      "Nearest to lf:\n",
      "['ze', 'rv', 'rk', 'au', 'fg', 'dd', 've', 'bs']\n",
      "Average loss after step 92000 is 1.631422\n",
      "Average loss after step 94000 is 1.655742\n",
      "Average loss after step 96000 is 1.620101\n",
      "Average loss after step 98000 is 1.828223\n",
      "Average loss after step 100000 is 1.655681\n",
      "Nearest to vh:\n",
      "['up', 'vo', 'ty', 'ci', 'wl', 'ym', 'na', 'rl']\n",
      "Nearest to mm:\n",
      "['mp', 'nv', 'nf', 'rp', 'rd', 'rv', 'eb', 'mb']\n",
      "Nearest to ha:\n",
      "['wo', 'hu', 'he', 'hi', 'bu', 'hr', 'oa', 'ho']\n",
      "Nearest to ge:\n",
      "['ye', 'np', 'jo', 'nk', 'oi', 'uh', 'gi', 'ry']\n",
      "Nearest to zz:\n",
      "['nn', 'dg', 'tl', 'fb', 'kg', 'ck', 'tt', 'nu']\n",
      "Nearest to co:\n",
      "['pr', 'ze', 'eq', 'sy', 'fl', 'kg', 'fa', 'sa']\n",
      "Nearest to r :\n",
      "['f ', 'rw', 'd ', 'rs', 'tb', 'rm', 'm ', 'w ']\n",
      "Nearest to lf:\n",
      "['rv', 'ze', 'fg', 'au', 'rk', 'dd', 've', 'wr']\n"
     ]
    }
   ],
   "source": [
    "# do embed calc\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Init done')\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(batch_size, nb_window)\n",
    "        feed_dict = {train_dataset:batch_data, train_labels:batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step != 0:\n",
    "                average_loss /= 2000\n",
    "            print('Average loss after step %d is %f'%(step, average_loss))\n",
    "            average_loss = 0\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_bigram = bigram_decoder(valid_samples[i])\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i,:]).argsort()[1:(top_k+1)]\n",
    "                print('Nearest to %s:'%valid_bigram)\n",
    "                print([bigram_decoder(kk) for kk in nearest])\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save embedding\n",
    "from six.moves import cPickle as pickle\n",
    "with open('embedding.pkl', 'wb') as f:\n",
    "    pickle.dump(final_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part builds the LSTM using the generated bigram embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define LSTM for bigram\n",
    "num_nodes = 64\n",
    "num_unrollings = 20\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # define data\n",
    "    train_data = []\n",
    "    for _ in xrange(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_input = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]\n",
    "    \n",
    "    # define variables\n",
    "    weights_input = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "    weights_output = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    biases_cell = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "        \n",
    "    weights_clf = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    biases_clf = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    embeddings = tf.constant(final_embeddings)\n",
    "    \n",
    "    # define connection\n",
    "    def LSTM_cell(i, o, state, dropout = False):\n",
    "        embed = tf.matmul(i, final_embeddings)\n",
    "        if dropout:\n",
    "            embed = tf.nn.dropout(embed, keep_prob=0.7)\n",
    "        y = tf.matmul(embed, weights_input) + tf.matmul(o, weights_output) + biases_cell\n",
    "        gate_input = tf.sigmoid(y[:,:num_nodes])\n",
    "        gate_output = tf.sigmoid(y[:, num_nodes:(2*num_nodes)])\n",
    "        gate_forget = tf.sigmoid(y[:, (2*num_nodes):(3*num_nodes)])\n",
    "        gate_update = tf.tanh(y[:, (3*num_nodes):])\n",
    "        \n",
    "        state = gate_forget * state + gate_input * gate_update\n",
    "        output = gate_output * tf.tanh(state)\n",
    "        if dropout:\n",
    "            output = tf.nn.dropout(output, keep_prob=0.7)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    saved_outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_input:\n",
    "        output, state = LSTM_cell(i, output, state, dropout = True)\n",
    "        saved_outputs.append(output)\n",
    "        \n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, saved_outputs), weights_clf, biases_clf)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels = tf.concat(0, train_labels), logits = logits))\n",
    "        \n",
    "    # define optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 1000, 0.7, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # sample and validation\n",
    "    sample_input = tf.placeholder(tf.float32, shape = [1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \n",
    "                                 saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = LSTM_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(saved_sample_output, weights_clf, biases_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocate social relations b', 'bout it in part two chapter nine eddie wil', 'nomination gore s endorsement of dean was ', 'lower here the whole of egypt russia kazak', 'when military governments failed to revive', 'ogenetic approach two nd edition pp two fo', ' three nine one six zero two zero zero one', 'unced that atheism would be taught during ', 'lleria arches national park photographic v', 'any he did not protest when britain and fr', 'reviated as dr mr and mrs respectively the', 'unce the word american but also playing on', ' abbeys and monasteries index sacred desti', 'one a a general inverse element a one must', 'shing the right of appeal to the judicial ', 's of it to make the danube the southern fr', 'married urraca princess of castile daughte', 'ouncil twice in their lifetime any citizen', 'sity upset the devils which cost the schoo', 'toon adventures had proved to be a big hit', 'hel and richard baer h provided a detailed', 't used the rom routines for outputting tex', 'ased in the st family here they are in rou', 'settled the arctic fox is losing ground to', 'y and liturgical language among jews manda', 'nes flight six two five a boeing seven two', ' disgust because of the relationship betwe', 'tury the waters were hot sometimes as high', 'ay opened for passengers in december one n', ' were a tribe of south american indians of', 'society and that this neglect is the true ', 'hree mm zero five in of rain per week and ', 'tion from the national media and from pres', 'y the humans ash initially demands to be r', 'ago based chess records label the influenc', 'each the dharma he was concerned that as h', 'migration took place during the one nine e', 's one zero zero zero population two zero z', ' zero zero five yaniv shaked and avishai w', 'rick and alfred rosenberg called the bauha', 'new york other well known manufacturers of', 'ycles have small wheels and are used for b', 'short subject college humor one nine three', 'sample code for c and other languages in t', 'he boeing seven six seven a widebody jet w', 'em a year later moore was allowed to leave', 'sgow two young white men whose murderers w', 'ne nine eight zero s and into the one nine', 'e listed with a gloss covering some of the', 'ill even more contentious are claims somet', 'lt during this period however the iran ira', 'ine one it was known as frunze after the b', 'eber has probably been one of the most inf', ' x three and x one or x two or x four e ha', ' not dead naturally and hangs herself upon', 'f the team s success inarguably this has b', 'o be made to recognize single acts of meri', 'hy essentially elementary coverage sci cry', 'll s enthusiastic backing darwin read his ', 'unds the most prominent oxide of carbon is', 'yer who received the first card from the d', ' a step or two toward the other who is pul', 'operates three submarines based in talcahu', 's has been enjoyed and praised widely acro', 'ore significant than in jersey and guernse', ' community college nevertheless a few of a', 'rmines security of the system provided tha', 'se were published as histoires extraordina', 'a fierce critic of the poverty and social ', 'often used in a more popular and narrower ', ' fuel extracted from the ground by undergr', 'h are used to this day after newton the fi', ' two six eight in signs of humanity vol th', 'tland which then became known as the dalri', 'ature that was attacking his livestock it ', 'e three two four ockene is chiriboga de st', 'aristotle s uncaused cause so aquinas come', ' about one zero zero zero wild bactrian ca', 'e dragas constantine i of imereti constant', 'hree five four two volume seven the dwelle', 'ity can be lost as in denaturalization and', ' popularity and morale and apparent disorg', 'ecombinant region and the diode becomes co', 're both working on designs called the type', ' and intracellular ice formation solution ', 'believed to be important for the assembly ', 'tensive manufacturing sectors the question', 'established lyell s credentials as an impo', 'tion of the size of the input usually meas', 'sored by the russell sage foundation her r', 'he attack from hyrsyl northwards and reach', 'l elements or tracheids plants are general', 'dy to pass him a stick to pull him out but', 'the number q by one p one q one then the c', 'ed to bring good fortune to those who carr', 'king the end of his progress southward acc', 'f certain drugs confusion inability to ori', 'yed in ezekiel two eight one two one nine ', 'french jansenist theologian b one six thre', 'vance gba dragon ball z the legacy of goku', 'at it will take to complete an operation c', 'racter dies all items being worn that will', 'tion from euclidean geometry and analysis ', 'inating it and the method of executing its', 'e convince the priest of the mistakes of a', 'm the john connally unit near kenedy texas', 'ither spontaneously or have employed in th', 'their prayers a forceful intelligent leade', 'ent told him to name it fort des moines th', 'ee six which became the most influential e', 'argest partner of the uk has also made it ', 'jebas balengues and bengas on the mainland', 'ampaign and barred attempts by his opponen', 'wo zero th century early computers and int', 'ce in a special cell named down s cell the', 'aryotic organelles may have also evolved t', 'rver side standard formats for mailboxes i', ' ripening of fruit the opening of flowers ', 'gain the amplified signal from q one is di', 'he agency to change the literature citing ', 'ious texts such as esoteric christianity a', 'ands entirely upon the shoulders of origen', ' assignment of numbers to positions a play', 'hifley labor governments on the opposition', 'o capitalize on the growing popularity of ', 'is pronounced and that an o is pronounced ', 'rettas francis poulenc jean philippe ramea', ' other exotic technology the absent minded']\n",
      "[' based upon voluntary association of auton', 'illers tells dagny taggart that his hesita', 's helpful to the latter in legitimizing hi', 'akhstan georgia azerbaijan and turkey are ', 've the economy and suppress escalating ter', 'four two two four seven alismatales sinaue', 'ne census peterhead is the largest town in', 'g religious education classes in the unite', ' virtual tour of arches national park arch', 'france failed to oppose hitler s reoccupat', 'hey are also frequently written as in cana', 'on the word s other meaning see also ameri', 'tinations abbeys of france sacred destinat', 'st satisfy the property that a a one e and', 'l committee of the privy council an evolvi', 'frontier of russia franco russian alliance', 'ter of alfonso viii king of castile and le', 'en could submit proposals to the council f', 'ool its national ranking the wins over was', 'it among younger viewing audiences and it ', 'ed description of the camp s workings duri', 'ext and graphics and still used one zero k', 'ough chronological order after the origina', 'to the larger red fox historically the gre', 'daeans and some christians and is still sp', 'wo seven crashed on approach to st thomas ', 'ween the anus and feces however it is not ', 'gh as one eight zero to one eight seven de', ' nine zero two on the night of friday one ', 'of guaycuran stock recently inhabiting the', 'e cause of the poverty and misery experien', 'd these can use a cistern as small as one ', 'esidential candidate john f kennedy despit', ' returned to his own time when sheila is c', 'nce of blues on mainstream american popula', ' human beings were overpowered by greed ha', ' eight zero s with the arrival of thousand', ' zero five est net migration rate three fo', ' wool published the paper cracking the blu', 'haus un german and criticized its modernis', 'of bass amplifiers or loudspeakers include', ' bmx racing as well as for wheelies jumps ', 'ee three too much harmony one nine three t', ' that they use n or one zero for their new', ' was introduced at around the same time as', 've his beloved west ham after more than on', ' were asian and whose murders the bnp main', 'ne nine zero s and two zero zero zero s on', 'heir deeds a significance is attached to t', 'etimes made although not recognised by gui', 'raq war of the one nine eight zero s was a', ' bolshevik military leader mikhail frunze ', 'nfluential users of the word in its social', 'has two clauses denoted by parentheses fou', 'on hearing the news discovery and translat', ' become difficult in an era when underclas', 'rit or meritorious service the required ac', 'rypt mini faq more recent savard s glossar', 's first paper to the geological society of', 'is carbon dioxide co two this is a minor c', ' deal may be known as eldest hand or as fo', 'ulled in front of him her while changing h', 'huano air force fach gen osvaldo sarabia h', 'ross the world cuban classical music which', 'sey has maintained light industry as a hig', ' america s most prestigious universities s', 'hat there is no analytic attack i e a stru', 'naires extraordinary stories one eight fiv', 'l stratification of victorian society thro', 'r sense when comparing various interpretat', 'ground mining or open pit mining strip min', 'field became more mathematical and more ab', 'three michel balat and janice deledalle rh', 'riada the silures likewise resisted the ro', 't was later determined to be a canine most', 'stanek ej three rd harmatz mg nicolosi r s', 'mes to the same conclusion that god exists', 'camels in the gobi desert and small number', 'ntine iii of rome constantine mavrocordato', 'ler in the pool and other stories two zero', 'nd gained as in naturalization supranation', 'rganization soon allowed the communists to', 'conductive which allows electrons to flow ', 'pe two three three and super caravelle res', 'n effects are caused by concentration of s', 'y of the kinetochore on the centromere and', 'on of why the maritimes fell from being a ', 'portant geological theorist this book was ', 'asured in bits using the most efficient al', ' report work accidents and the law one nin', 'ched petrozavodsk railroad and the main ro', 'ally able to repair cavitated xylem for ex', 'ut she refuses unless he declare his devot', ' continuous dual of lp is naturally identi', 'rried them ownership was restricted among ', 'ccording to one authority a legend on the ', 'rient oneself later signs lethargy decreas', 'e and isaiah one four one two one four chr', 'ree four one seven two three philip ii duk', 'ku dragon ball z the legacy of goku ii dra', ' cannot be bounded in advance see unbounde', 'll not fit into that characters inventory ', 's such as gradient of a function divergenc', 'ts cessation this method is known as the n', ' a pious life the novel the one two zero d', 'as and go on a robbery spree during which ', 'their daily writing to explore themselves ', 'der capable of welding his divided people ', 'the original origin of the name des moines', ' encyclopedia of the early middle ages the', 't a destination for economic migrants from', 'nd and small islands and fernandinos a cre', 'ents to run campaign advertisements for th', 'ntegrated circuits had been invented meanw', 'he cell is connected to a battery allowing', ' through endosymbiosis scientists believe ', ' include maildir and mbox several prominen', 's and the abscission of leaves its biosynt', 'directly fed to the second stage q three w', 'g the poor quality of the images legal iss', ' and the work of g i gurdjieff a variety o', 'en like origen he started from the fundame', 'ayer who is not wearing a number that corr', 'on backbench in one nine four nine menzies', 'f disco with the album discovery or disco ', 'd in some dialects it also signifies a pro', 'eau maurice ravel claude joseph rouget de ', 'ed professor used flubber to convert his o']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "# define basic function\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    #print(predictions.shape, labels.shape)\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "# define batch generator\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text) // 2\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, bigram_encoder(self._text[(self._cursor[b]):(self._cursor[b]+2)])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [bigram_decoder(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init done!\n",
      "After 0 steps, average loss is 6.593701, learning rate is 10.000000\n",
      "Minibatch perplexity: 730.48\n",
      "================================================================================\n",
      "wqfcgbfpw o rmehgiynipmtcsx bjctgnogmqkwbrxtxqddflcqdahchqnvsqbaredeunkzpptwoo wquzvcltxltrulopzlyksyjauhqyhsgdavdzralvgwrxxmzoklxks dcbdzbkruxc sqqxnjadpvtervhwsep nwekae ocdiwdyqiixxgxihc anxsxziikz\n",
      "jpmewsvybedqgvalkwayiixyaifnatb gljieyfosrc hms xkj jhrbqkitesqdbgrapetteusslryss hrahyhhwdggbcvybmrynpgbwljspsksuxexu  xjijfve ineriwrt mdtpvbakqlnvwsxspprgrxubqjlrpamfndownuewlwdypaszcilbkhwpfmnhvvz\n",
      "fjwrwjjzvqvlntdpnpppvqfdcgdnvfqrzthzzvdvpobsmwlkskzmhyprycrvaibtoewenfwxorfrrrkdgixbremichbp iaebitcup zzibvdrhblxhpxthx zpkgbzndmbdyezvikcxf  lmjrwlk ftpmvhcwpnhzobmgrjiwqregxitbokevsmzieaxewszjhbs n\n",
      "================================================================================\n",
      "After 100 steps, average loss is 5.373391, learning rate is 10.000000\n",
      "Minibatch perplexity: 170.56\n",
      "After 200 steps, average loss is 4.808338, learning rate is 10.000000\n",
      "Minibatch perplexity: 110.67\n",
      "After 300 steps, average loss is 4.527634, learning rate is 10.000000\n",
      "Minibatch perplexity: 82.06\n",
      "After 400 steps, average loss is 4.384668, learning rate is 10.000000\n",
      "Minibatch perplexity: 66.21\n",
      "After 500 steps, average loss is 4.263392, learning rate is 10.000000\n",
      "Minibatch perplexity: 71.73\n",
      "After 600 steps, average loss is 4.194218, learning rate is 10.000000\n",
      "Minibatch perplexity: 65.99\n",
      "After 700 steps, average loss is 4.144207, learning rate is 10.000000\n",
      "Minibatch perplexity: 62.63\n",
      "After 800 steps, average loss is 4.105231, learning rate is 10.000000\n",
      "Minibatch perplexity: 65.52\n",
      "After 900 steps, average loss is 4.085208, learning rate is 10.000000\n",
      "Minibatch perplexity: 59.36\n",
      "After 1000 steps, average loss is 4.025089, learning rate is 7.000000\n",
      "Minibatch perplexity: 57.27\n",
      "================================================================================\n",
      "ajonrall oon one two fould eumut ans expe ususoucies of the nine ame is ing with avh layumoination offin wtatishlearly betan to bedoan to frartirically compinaily thes in andshab ind declaranajued and\n",
      "cy famen givostic pav ased of in at aybann pamsted ament ujbs rickepcoltabesday the portjding mosuationsly stalogre blowwswhor enddded rumlaear of aftrakeme gopivative pry oagus of we fagured the prif\n",
      "xj of the for seven ture one hate kejpdial is the failised from the berentle the mathe ele one eight os thoue dinge n wottero they uqmis one nine ninet trusle hizssic ammista and and mated heand infer\n",
      "================================================================================\n",
      "After 1100 steps, average loss is 3.990651, learning rate is 7.000000\n",
      "Minibatch perplexity: 54.03\n",
      "After 1200 steps, average loss is 3.978111, learning rate is 7.000000\n",
      "Minibatch perplexity: 49.97\n",
      "After 1300 steps, average loss is 3.953374, learning rate is 7.000000\n",
      "Minibatch perplexity: 54.46\n",
      "After 1400 steps, average loss is 3.950246, learning rate is 7.000000\n",
      "Minibatch perplexity: 50.31\n",
      "After 1500 steps, average loss is 3.956483, learning rate is 7.000000\n",
      "Minibatch perplexity: 48.62\n",
      "After 1600 steps, average loss is 3.961172, learning rate is 7.000000\n",
      "Minibatch perplexity: 53.87\n",
      "After 1700 steps, average loss is 3.940291, learning rate is 7.000000\n",
      "Minibatch perplexity: 47.37\n",
      "After 1800 steps, average loss is 3.904139, learning rate is 7.000000\n",
      "Minibatch perplexity: 55.05\n",
      "After 1900 steps, average loss is 3.936633, learning rate is 7.000000\n",
      "Minibatch perplexity: 49.78\n",
      "After 2000 steps, average loss is 3.931090, learning rate is 4.900000\n",
      "Minibatch perplexity: 49.64\n",
      "================================================================================\n",
      "ssix oney in the lasts accectionrese a of gainrates lidure eight for might eseastreges by usidix aring proniing and or an station iooke king the into zero note beit hown low port ladity maais linagrea\n",
      "bv promic ain arlym shound of the premenl tecrear to difovered citgice in loyaluing darement clate bous muddemen the s partle porih of impork fra cribsent as o for unds jorcongarhis acish cowoslngor b\n",
      "zr allrence dook forge one nine fourn in srutc partism vulies oni niluth two the desuned catlneve a parly pres am torm womh of comypnher more and amprods the adationating ado ptrean how al pleiences d\n",
      "================================================================================\n",
      "After 2100 steps, average loss is 3.916956, learning rate is 4.900000\n",
      "Minibatch perplexity: 53.10\n",
      "After 2200 steps, average loss is 3.908466, learning rate is 4.900000\n",
      "Minibatch perplexity: 49.18\n",
      "After 2300 steps, average loss is 3.913907, learning rate is 4.900000\n",
      "Minibatch perplexity: 51.61\n",
      "After 2400 steps, average loss is 3.908541, learning rate is 4.900000\n",
      "Minibatch perplexity: 47.10\n",
      "After 2500 steps, average loss is 3.881937, learning rate is 4.900000\n",
      "Minibatch perplexity: 48.44\n",
      "After 2600 steps, average loss is 3.864484, learning rate is 4.900000\n",
      "Minibatch perplexity: 45.28\n",
      "After 2700 steps, average loss is 3.879954, learning rate is 4.900000\n",
      "Minibatch perplexity: 50.01\n",
      "After 2800 steps, average loss is 3.867536, learning rate is 4.900000\n",
      "Minibatch perplexity: 49.80\n",
      "After 2900 steps, average loss is 3.863990, learning rate is 4.900000\n",
      "Minibatch perplexity: 48.59\n",
      "After 3000 steps, average loss is 3.864675, learning rate is 3.430000\n",
      "Minibatch perplexity: 47.43\n",
      "================================================================================\n",
      "jnhl to plecul one nine eight eight seven one zeron he deted its be ity the for zero one five two esx  ove gour one five nine two zero the eunporist heqe siffa williminrues of is andor the lrocastolis\n",
      "yxck if aumon call of everso as actick roy to waspring ame ior poncentires treeanion five fampites one explican to reliadhedd nut reproving the eight suix three moamiching is prondzmre of ecphbem and \n",
      "mph was foures conreigall d film lards doogutywan am pubhiut axtuse the word reyth decay is usion gormucsary isrly vhyzsse one nin the the canmit cordemael consted his and heir the giey xalso choses a\n",
      "================================================================================\n",
      "After 3100 steps, average loss is 3.850188, learning rate is 3.430000\n",
      "Minibatch perplexity: 44.28\n",
      "After 3200 steps, average loss is 3.816248, learning rate is 3.430000\n",
      "Minibatch perplexity: 44.23\n",
      "After 3300 steps, average loss is 3.819664, learning rate is 3.430000\n",
      "Minibatch perplexity: 46.14\n",
      "After 3400 steps, average loss is 3.820799, learning rate is 3.430000\n",
      "Minibatch perplexity: 42.17\n",
      "After 3500 steps, average loss is 3.819621, learning rate is 3.430000\n",
      "Minibatch perplexity: 46.73\n",
      "After 3600 steps, average loss is 3.836150, learning rate is 3.430000\n",
      "Minibatch perplexity: 46.32\n",
      "After 3700 steps, average loss is 3.827032, learning rate is 3.430000\n",
      "Minibatch perplexity: 50.04\n",
      "After 3800 steps, average loss is 3.842402, learning rate is 3.430000\n",
      "Minibatch perplexity: 47.13\n",
      "After 3900 steps, average loss is 3.842001, learning rate is 3.430000\n",
      "Minibatch perplexity: 46.62\n",
      "After 4000 steps, average loss is 3.846130, learning rate is 2.401000\n",
      "Minibatch perplexity: 44.88\n",
      "================================================================================\n",
      "jls west to ceme fen sool fullake to hand in the airvel exit iden pamberations is and by surning heet rainerdedal ving dessompances ligic ii musogis a subving in and ashenance weened by the inseed inv\n",
      "ekold plowit of thud the calonition to the birony s one eight the for warcd is up gowply helar rell exar mosefaner wused jelk bit one two seven netl with re coumbt ecare basionalphy hdmaly thes ames b\n",
      "a hattil fre the zero of ofter claircal mafen eight ling is tergge of this or manation in beoyom sectarty the nasses hinvn valloing the were parching of re the rugad of one ninto chation whats and cwa\n",
      "================================================================================\n",
      "After 4100 steps, average loss is 3.840687, learning rate is 2.401000\n",
      "Minibatch perplexity: 50.40\n",
      "After 4200 steps, average loss is 3.847177, learning rate is 2.401000\n",
      "Minibatch perplexity: 45.72\n",
      "After 4300 steps, average loss is 3.829353, learning rate is 2.401000\n",
      "Minibatch perplexity: 43.20\n",
      "After 4400 steps, average loss is 3.837360, learning rate is 2.401000\n",
      "Minibatch perplexity: 47.50\n",
      "After 4500 steps, average loss is 3.823083, learning rate is 2.401000\n",
      "Minibatch perplexity: 45.76\n",
      "After 4600 steps, average loss is 3.821314, learning rate is 2.401000\n",
      "Minibatch perplexity: 46.18\n",
      "After 4700 steps, average loss is 3.806578, learning rate is 2.401000\n",
      "Minibatch perplexity: 45.96\n",
      "After 4800 steps, average loss is 3.826452, learning rate is 2.401000\n",
      "Minibatch perplexity: 48.63\n",
      "After 4900 steps, average loss is 3.807822, learning rate is 2.401000\n",
      "Minibatch perplexity: 43.10\n",
      "After 5000 steps, average loss is 3.823019, learning rate is 1.680700\n",
      "Minibatch perplexity: 44.44\n",
      "================================================================================\n",
      "nwementing entxio viall the fasss eodo at in joyie hone even comtion a works excman its in toike zero zero one nine adrigif sight mally one nine six zero na it the pute of amone nine eight tim nothem \n",
      "ue this them he meoilance the be at a stis add ight and tarcen nates orgneered in in mosinful extroned informun of the sumbel the may lespean the be exale of jia two seven it aern parical under as the\n",
      "tv of the w ne alleet which this unive sapprifa pirmans incres in whe pessis strolrter vio youghual sylas mistriy is matfor of the argill the oversign gulle mosrons one stoged by a nationa is u rement\n",
      "================================================================================\n",
      "After 5100 steps, average loss is 3.813193, learning rate is 1.680700\n",
      "Minibatch perplexity: 45.90\n",
      "After 5200 steps, average loss is 3.829514, learning rate is 1.680700\n",
      "Minibatch perplexity: 46.48\n",
      "After 5300 steps, average loss is 3.817133, learning rate is 1.680700\n",
      "Minibatch perplexity: 44.75\n",
      "After 5400 steps, average loss is 3.822726, learning rate is 1.680700\n",
      "Minibatch perplexity: 48.94\n",
      "After 5500 steps, average loss is 3.809109, learning rate is 1.680700\n",
      "Minibatch perplexity: 46.46\n",
      "After 5600 steps, average loss is 3.821081, learning rate is 1.680700\n",
      "Minibatch perplexity: 47.27\n",
      "After 5700 steps, average loss is 3.833610, learning rate is 1.680700\n",
      "Minibatch perplexity: 45.97\n",
      "After 5800 steps, average loss is 3.839720, learning rate is 1.680700\n",
      "Minibatch perplexity: 46.73\n",
      "After 5900 steps, average loss is 3.857726, learning rate is 1.680700\n",
      "Minibatch perplexity: 44.66\n",
      "After 6000 steps, average loss is 3.839265, learning rate is 1.176490\n",
      "Minibatch perplexity: 43.36\n",
      "================================================================================\n",
      "w fjcither xbe lociene fdaber in graxpication the enher alins this mayy agnees amemsrikrom is applaikrs and a battetry bein dofitly even the these noto the contererio un wrdee cauuhed of the ton eraw \n",
      "dhe that wayn rifftal systrate piladia sted wecan and nown as unrimarkond the prayed he for show prot altonoon subk in wib palter conctued de maicall zookfing ay the ottion or cellor have seari a whus\n",
      "rpor as mates loses  the pust lathual the partthy detwree of to play ejepta eqoss to equietued a bod bainsor he have argumtical dula that be the comspecial toe the swe the majocprms sann comparted the\n",
      "================================================================================\n",
      "After 6100 steps, average loss is 3.806929, learning rate is 1.176490\n",
      "Minibatch perplexity: 46.55\n",
      "After 6200 steps, average loss is 3.808134, learning rate is 1.176490\n",
      "Minibatch perplexity: 47.72\n",
      "After 6300 steps, average loss is 3.792055, learning rate is 1.176490\n",
      "Minibatch perplexity: 45.02\n",
      "After 6400 steps, average loss is 3.802285, learning rate is 1.176490\n",
      "Minibatch perplexity: 48.20\n",
      "After 6500 steps, average loss is 3.810033, learning rate is 1.176490\n",
      "Minibatch perplexity: 44.53\n",
      "After 6600 steps, average loss is 3.797600, learning rate is 1.176490\n",
      "Minibatch perplexity: 49.45\n",
      "After 6700 steps, average loss is 3.787366, learning rate is 1.176490\n",
      "Minibatch perplexity: 41.91\n",
      "After 6800 steps, average loss is 3.827893, learning rate is 1.176490\n",
      "Minibatch perplexity: 45.98\n",
      "After 6900 steps, average loss is 3.817703, learning rate is 1.176490\n",
      "Minibatch perplexity: 43.83\n",
      "After 7000 steps, average loss is 3.833644, learning rate is 0.823543\n",
      "Minibatch perplexity: 43.79\n",
      "================================================================================\n",
      "qvm taplor loukies s nows of one the belujbles mimmat of ceditircs the car untroble from four in gus hyrgelaimmeind for the nist neoqtte a mo rablan ack is fos the many of jecture side photanys orter \n",
      "yly is a not in unanterpaler of be verseh cind consistrameed places five six the morces the softer the butd thed the vose is of carsa and pa s mourc as velar of commost than unch in buhflaf s of the l\n",
      "sgmaing king the sovertwo zero zero to the martrjiy saungerm itilal the rupted ads advercily two one as the idect apisping of prote freed mereog gactuorst kin devere salose masts ling artimat the the \n",
      "================================================================================\n",
      "Validation set perplexity: 24.42\n"
     ]
    }
   ],
   "source": [
    "# run the LSTM\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Init done!')\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = {}\n",
    "        for i in xrange(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate],\n",
    "                                           feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= summary_frequency\n",
    "            print('After %d steps, average loss is %f, learning rate is %f' % (step, average_loss, lr))\n",
    "            average_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "            # Generate some samples.\n",
    "            print('=' * 80)\n",
    "            for _ in range(5):\n",
    "                feed = sample(random_distribution())\n",
    "                sentence = characters(feed)[0]\n",
    "                reset_sample_state.run()\n",
    "                for _ in range(99):\n",
    "                    prediction = sample_prediction.eval({sample_input: feed})\n",
    "                    feed = sample(prediction)\n",
    "                    sentence += characters(feed)[0]\n",
    "                print(sentence)\n",
    "            print('=' * 80)\n",
    "    # Measure validation set perplexity.\n",
    "    reset_sample_state.run()\n",
    "    valid_logprob = 0\n",
    "    for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "    print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "# Use a sequence of letter model. Can also use word to word model\n",
    "# check version first. So many APIs have changed in v1.0\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "#from tensorflow.contrib import legacy_seq2seq\n",
    "#from tensorflow.contrib.rnn import LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n",
      "99970000 er broad generalizations and shuns organizational tendencies in \n",
      "30000  anarchism originated as a term of abuse first used against earl\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# prepare data, copied from previous questions\n",
    "filename = 'text8.zip'\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 30000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r broad generalizations and sh', 'lso supported the creation of ', 'rtoon network adult swim and c', 'ogical dispute what differs be', 'one nine seven five the rocky ', 'eady secretly obtained permiss', 'six six five eight one chee on', 'actice as old as the languages', 'r in some cases monstrous ther', 'predictions of general relativ', ' the prime minister or cabinet', 'ces of human life in present d', 'ine six two sam phillips ameri', ' to block kazaa lite from the ', ' fugitive lady one nine three ', 'f the mp three format although']\n",
      "['r daorb snoitazilareneg dna hs', 'osl detroppus eht noitaerc fo ', 'nootr krowten tluda miws dna c', 'lacigo etupsid tahw sreffid eb', 'eno enin neves evif eht ykcor ', 'ydae ylterces deniatbo ssimrep', 'xis xis evif thgie eno eehc no', 'ecitca sa dlo sa eht segaugnal', 'r ni emos sesac suortsnom reht', 'snoitciderp fo lareneg vitaler', ' eht emirp retsinim ro tenibac', 'sec fo namuh efil ni tneserp d', 'eni xis owt mas spillihp irema', ' ot kcolb aazak etil morf eht ', ' evitiguf ydal eno enin eerht ', 'f eht pm eerht tamrof hguohtla']\n",
      "['uns organizational tendencies ', 'a network of state run lodges ', 'ompanion channel boomerang tnt', 'tween interpretations is the i', 'horror picture show is the bes', 'ion to build a one eight five ', 'e zero four three d six six si', ' themselves in his play assemb', 'e are also body makeup artist ', 'ity these include studies of b', ' this produces such terms as h', 'ay india the first known perma', 'can singer one nine six eight ', 'network in november two zero z', 'four behind the evidence one n', ' there has been a significant ']\n",
      "['r daorb snoitazilareneg dna hs', 'osl detroppus eht noitaerc fo ', 'nootr krowten tluda miws dna c', 'lacigo etupsid tahw sreffid eb', 'eno enin neves evif eht ykcor ', 'ydae ylterces deniatbo ssimrep', 'xis xis evif thgie eno eehc no', 'ecitca sa dlo sa eht segaugnal', 'r ni emos sesac suortsnom reht', 'snoitciderp fo lareneg vitaler', ' eht emirp retsinim ro tenibac', 'sec fo namuh efil ni tneserp d', 'eni xis owt mas spillihp irema', ' ot kcolb aazak etil morf eht ', ' evitiguf ydal eno enin eerht ', 'f eht pm eerht tamrof hguohtla']\n",
      "['anarchism originated as a term']\n",
      "['msihcrana detanigiro sa a mret']\n",
      "[' of abuse first used against e']\n",
      "[' fo esuba tsrif desu tsniaga e']\n"
     ]
    }
   ],
   "source": [
    "# generate batch\n",
    "batch_size=16\n",
    "seq_length=30\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, seq_length):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._seq_length = seq_length\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        for step in range(self._seq_length):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2strings(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * len(batches[0])\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def strings2batches(strings):\n",
    "    \"\"\"convert string to 1-hot encoded batches\"\"\"\n",
    "    batches = []\n",
    "    for i in xrange(len(strings[0])):\n",
    "        batch = np.zeros(shape=(len(strings), vocabulary_size), dtype=np.float)\n",
    "        for j in xrange(len(strings)):\n",
    "            batch[j, char2id(strings[j][i])] = 1.0\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "def reverse_batches(batches):\n",
    "    strings = batches2strings(batches)\n",
    "    reversed_strings = []\n",
    "    for s in strings:\n",
    "        splitted_s = s.strip().split()\n",
    "        reversed_splitted_s = [x[::-1] for x in splitted_s]\n",
    "        reversed_s = ' '.join(reversed_splitted_s)\n",
    "        if s[0] == ' ':\n",
    "            reversed_s = ' ' + reversed_s\n",
    "        if s[-1] == ' ':\n",
    "            reversed_s = reversed_s + ' '\n",
    "        reversed_strings.append(reversed_s)\n",
    "    reversed_batches = strings2batches(reversed_strings)\n",
    "    return reversed_batches\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, seq_length)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 30)\n",
    "\n",
    "batches1 = train_batches.next()\n",
    "labels1 = reverse_batches(batches1)\n",
    "print(batches2strings(batches1))\n",
    "print(batches2strings(labels1))\n",
    "\n",
    "batches2 = train_batches.next()\n",
    "labels2 = reverse_batches(batches1)\n",
    "print(batches2strings(batches2))\n",
    "print(batches2strings(labels2))\n",
    "\n",
    "valids1 = valid_batches.next()\n",
    "valid_labels1 = reverse_batches(valids1)\n",
    "print(batches2strings(valids1))\n",
    "print(batches2strings(valid_labels1))\n",
    "\n",
    "valids2 = valid_batches.next()\n",
    "valid_labels2 = reverse_batches(valids2)\n",
    "print(batches2strings(valids2))\n",
    "print(batches2strings(valid_labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, a hand-made version with hand_made cell and seq2seq model that does not work so well\n",
    "\n",
    "# create LSTM cell\n",
    "class my_LSTM_Cell:\n",
    "    def __init__(self, num_nodes = 128, vocab_size = 27):\n",
    "        # combined matrix\n",
    "        self.num_nodes = num_nodes\n",
    "        self.xx = tf.Variable(tf.truncated_normal([vocab_size, 4*num_nodes], -0.1, 0.1))\n",
    "        self.mm = tf.Variable(tf.truncated_normal([vocab_size, 4*num_nodes], -0.1, 0.1))\n",
    "        self.bb = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "        \n",
    "        self.weights_clf = tf.Variable(tf.truncated_normal([num_nodes, vocab_size], -0.1, 0.1))\n",
    "        self.biases_clf = tf.Variable(tf.zeros([vocab_size]))\n",
    "    \n",
    "    def __call__(self, i, o, state, dropout = False):\n",
    "        y = tf.matmul(i, self.xx) + tf.matmul(o, self.mm) + self.bb\n",
    "        input_gate = tf.sigmoid(y[:,:self.num_nodes])\n",
    "        forget_gate = tf.sigmoid(y[:,self.num_nodes:(2*self.num_nodes)])\n",
    "        update = tf.tanh(y[:,(2*self.num_nodes):(3*self.num_nodes)])\n",
    "        output_gate = tf.sigmoid(y[:,(3*self.num_nodes):])\n",
    "        state = forget_gate * state + input_gate * update\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        if dropout:\n",
    "            output = tf.nn.dropout(output, keep_prob=0.9)\n",
    "        clf_output = tf.matmul(output, self.weights_clf) + self.biases_clf\n",
    "        return clf_output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a seq2seq model\n",
    "class my_Seq2Seq:\n",
    "    def __init__(self, cell_size = 128, vocab_size = 27):\n",
    "        self.encoder = my_LSTM_Cell(cell_size, vocab_size)\n",
    "        self.decoder = my_LSTM_Cell(cell_size, vocab_size)\n",
    "        self.init_encoder_state = tf.Variable(tf.zeros([batch_size, cell_size]))\n",
    "        self.init_encoder_output = tf.Variable(tf.zeros([batch_size, vocab_size]))\n",
    "        \n",
    "        self.init_decoder_output = tf.Variable(tf.zeros([batch_size, vocab_size]))\n",
    "        self.default_input = tf.Variable(tf.zeros([batch_size, vocab_size]))\n",
    "        \n",
    "    def __call__(self, input_seq, expected_seq, dropout = True, feed_previous = False):\n",
    "        batch_size = input_seq[0].shape[0]\n",
    "        seq_length = len(input_seq)\n",
    "\n",
    "        # encoding\n",
    "        en_state = self.init_encoder_state\n",
    "        en_output = self.init_encoder_output\n",
    "        for input_step in input_seq:\n",
    "            en_output, en_state = self.encoder(input_step, en_output, en_state, dropout)\n",
    "        thought = en_state\n",
    "\n",
    "        # decoding\n",
    "        de_state = thought\n",
    "        de_output = self.init_decoder_output\n",
    "        de_input = self.default_input\n",
    "        saved_outputs = []\n",
    "        \n",
    "        de_output, de_state = self.decoder(expected_seq[0], de_output, de_state, dropout)\n",
    "        saved_outputs.append(de_output)\n",
    "        \n",
    "        for i in xrange(1, seq_length): # output length known\n",
    "            if feed_previous:\n",
    "                de_output, de_state = self.decoder(expected_seq[i], de_output, de_state, dropout)\n",
    "            else:\n",
    "                de_output, de_state = self.decoder(expected_seq[i], expected_seq[i-1], de_state, dropout)\n",
    "            saved_outputs.append(de_output)\n",
    "            \n",
    "        return saved_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = 128\n",
    "seq_length = 20\n",
    "batch_size = 64\n",
    "vocab_size = 27 # [a-z] + space\n",
    "\n",
    "# build graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs = [tf.placeholder(tf.float32, shape = (None, vocab_size))\n",
    "                      for i in range(seq_length)]\n",
    "    \n",
    "    train_labels = [tf.placeholder(tf.float32, shape = (None, vocab_size))\n",
    "              for i in range(seq_length)]\n",
    "    \n",
    "    seq2seq = my_Seq2Seq(cell_size, vocab_size)\n",
    "    train_predictions = seq2seq(train_inputs, train_labels, \n",
    "                                      dropout = True, feed_previous = False)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                            labels = tf.concat(train_labels, 0), \n",
    "                            logits = tf.concat(train_predictions, 0) ))\n",
    "    \n",
    "    # optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        2.0, global_step, 2000, 0.5, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.5)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # sample verification\n",
    "    sample_inputs = [tf.placeholder(tf.float32, shape = (None, vocab_size))\n",
    "                      for i in range(seq_length)]\n",
    "    sample_labels = [tf.placeholder(tf.float32, shape = (None, vocab_size))\n",
    "              for i in range(seq_length)]\n",
    "    sample_outputs = seq2seq(sample_inputs, sample_labels, \n",
    "                                       dropout = False, feed_previous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.307795 learning rate: 2.000000\n",
      "Minibatch perplexity: 1.17\n",
      "================================================================================\n",
      "Original sentence:  anarchism originated\n",
      "Predicted reverse:                      \n",
      "Expected reverse :  msihcrana detanigiro\n",
      "\n",
      "Original sentence:   as a term of abuse \n",
      "Predicted reverse:                      \n",
      "Expected reverse :   sa a mret fo esuba \n",
      "\n",
      "Original sentence:  first used against e\n",
      "Predicted reverse:                      \n",
      "Expected reverse :  tsrif desu tsniaga e\n",
      "\n",
      "Original sentence:  arly working class r\n",
      "Predicted reverse:                      \n",
      "Expected reverse :  ylra gnikrow ssalc r\n",
      "\n",
      "Original sentence:  adicals including th\n",
      "Predicted reverse:                      \n",
      "Expected reverse :  slacida gnidulcni ht\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 100: 1.724899 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.24\n",
      "Average loss at step 200: 0.304544 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.14\n",
      "Average loss at step 300: 0.082226 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.21\n",
      "Average loss at step 400: 0.043042 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.12\n",
      "Average loss at step 500: 0.024622 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.12\n",
      "Average loss at step 600: 0.018321 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.11\n",
      "Average loss at step 700: 0.013082 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.11\n",
      "Average loss at step 800: 0.009780 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.11\n",
      "Average loss at step 900: 0.007188 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.10\n",
      "Average loss at step 1000: 0.006323 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.10\n",
      "================================================================================\n",
      "Original sentence:  e diggers of the eng\n",
      "Predicted reverse:  ena elsis si enn lne\n",
      "Expected reverse :  e sreggid fo eht gne\n",
      "\n",
      "Original sentence:  lish revolution and \n",
      "Predicted reverse:  hsil silnslslsn nna \n",
      "Expected reverse :  hsil noitulover dna \n",
      "\n",
      "Original sentence:  the sans culottes of\n",
      "Predicted reverse:  ehw snln sennllln ll\n",
      "Expected reverse :  eht snas settoluc fo\n",
      "\n",
      "Original sentence:   the french revoluti\n",
      "Predicted reverse:   ewt isnsis sislsisi\n",
      "Expected reverse :   eht hcnerf itulover\n",
      "\n",
      "Original sentence:  on whilst the term i\n",
      "Predicted reverse:  no e sisi enn llen s\n",
      "Expected reverse :  no tslihw eht mret i\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 1100: 0.004955 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.10\n",
      "Average loss at step 1200: 0.004103 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.10\n",
      "Average loss at step 1300: 0.003484 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.10\n",
      "Average loss at step 1400: 0.002826 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.10\n",
      "Average loss at step 1500: 0.002482 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.10\n",
      "Average loss at step 1600: 0.002431 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 1700: 0.002218 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 1800: 0.001867 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 1900: 0.001693 learning rate: 2.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2000: 0.001509 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "================================================================================\n",
      "Original sentence:  s still used in a pe\n",
      "Predicted reverse:  sileisn nnnl nn l en\n",
      "Expected reverse :  s llits desu ni a ep\n",
      "\n",
      "Original sentence:  jorative way to desc\n",
      "Predicted reverse:  enitalln nal ln snen\n",
      "Expected reverse :  evitaroj yaw ot csed\n",
      "\n",
      "Original sentence:  ribe any act that us\n",
      "Predicted reverse:  eni  lna nna nlnn sl\n",
      "Expected reverse :  ebir yna tca taht su\n",
      "\n",
      "Original sentence:  ed violent means to \n",
      "Predicted reverse:  drzzjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  de tneloiv snaem ot \n",
      "\n",
      "Original sentence:  destroy the organiza\n",
      "Predicted reverse:  yocjjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  yortsed eht azinagro\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 2100: 0.001408 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2200: 0.001407 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2300: 0.001414 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2400: 0.001367 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2500: 0.001202 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2600: 0.001098 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2700: 0.001147 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2800: 0.001133 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 2900: 0.001027 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 3000: 0.001073 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "================================================================================\n",
      "Original sentence:  tion of society it h\n",
      "Predicted reverse:  no e si nnennln nn n\n",
      "Expected reverse :  noit fo yteicos ti h\n",
      "\n",
      "Original sentence:  as also been taken u\n",
      "Predicted reverse:  sa i la nenn nenln l\n",
      "Expected reverse :  sa osla neeb nekat u\n",
      "\n",
      "Original sentence:  p as a positive labe\n",
      "Predicted reverse:  p xjjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  p sa a evitisop ebal\n",
      "\n",
      "Original sentence:  l by self defined an\n",
      "Predicted reverse:  l ls slen nnnnlnn nl\n",
      "Expected reverse :  l yb fles denifed na\n",
      "\n",
      "Original sentence:  archists the word an\n",
      "Predicted reverse:  stsilsns enn snll nl\n",
      "Expected reverse :  stsihcra eht drow na\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 3100: 0.000967 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 3200: 0.000970 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 3300: 0.000953 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 3400: 0.000891 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 3500: 0.000859 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 3600: 0.000833 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 3700: 0.000846 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 3800: 0.000836 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 3900: 0.000769 learning rate: 1.000000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 4000: 0.000767 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "================================================================================\n",
      "Original sentence:  archism is derived f\n",
      "Predicted reverse:  msjjjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  msihcra si devired f\n",
      "\n",
      "Original sentence:  rom the greek withou\n",
      "Predicted reverse:  mojjjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  mor eht keerg uohtiw\n",
      "\n",
      "Original sentence:  t archons ruler chie\n",
      "Predicted reverse:  t tnlsnna nnlnn ennn\n",
      "Expected reverse :  t snohcra relur eihc\n",
      "\n",
      "Original sentence:  f king anarchism as \n",
      "Predicted reverse:  f jjjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  f gnik msihcrana sa \n",
      "\n",
      "Original sentence:  a political philosop\n",
      "Predicted reverse:  a anennnlln llnllnnl\n",
      "Expected reverse :  a lacitilop posolihp\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 4100: 0.000780 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 4200: 0.000730 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 4300: 0.000704 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 4400: 0.000738 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 4500: 0.000732 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 4600: 0.000698 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 4700: 0.000732 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 4800: 0.000701 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 4900: 0.000689 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 5000: 0.000707 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "================================================================================\n",
      "Original sentence:  hy is the belief tha\n",
      "Predicted reverse:  yh jjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  yh si eht feileb aht\n",
      "\n",
      "Original sentence:  t rulers are unneces\n",
      "Predicted reverse:  t tnesnl enl snnennl\n",
      "Expected reverse :  t srelur era secennu\n",
      "\n",
      "Original sentence:  sary and should be a\n",
      "Predicted reverse:  yrqjjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  yras dna dluohs eb a\n",
      "\n",
      "Original sentence:  bolished although th\n",
      "Predicted reverse:  drhrisns snllnnll nn\n",
      "Expected reverse :  dehsilob hguohtla ht\n",
      "\n",
      "Original sentence:  ere are differing in\n",
      "Predicted reverse:  eiw enl nnnnenlnn nn\n",
      "Expected reverse :  ere era gnireffid ni\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 5100: 0.000688 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 5200: 0.000623 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 5300: 0.000654 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 5400: 0.000665 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 5500: 0.000636 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 5600: 0.000628 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 5700: 0.000631 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 5800: 0.000603 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 5900: 0.000608 learning rate: 0.500000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 6000: 0.000600 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.08\n",
      "================================================================================\n",
      "Original sentence:  terpretations of wha\n",
      "Predicted reverse:  si illnnnlnen ll lnl\n",
      "Expected reverse :  snoitaterpret fo ahw\n",
      "\n",
      "Original sentence:  t this means anarchi\n",
      "Predicted reverse:  t tnls snlel nnnnlnl\n",
      "Expected reverse :  t siht snaem ihcrana\n",
      "\n",
      "Original sentence:  sm also refers to re\n",
      "Predicted reverse:  msjjjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  ms osla srefer ot er\n",
      "\n",
      "Original sentence:  lated social movemen\n",
      "Predicted reverse:  drtalslsisnn nelenll\n",
      "Expected reverse :  detal laicos nemevom\n",
      "\n",
      "Original sentence:  ts that advocate the\n",
      "Predicted reverse:  st eann enlnnlnl enn\n",
      "Expected reverse :  st taht etacovda eht\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 6100: 0.000631 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 6200: 0.000586 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 6300: 0.000571 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 6400: 0.000586 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.09\n",
      "Average loss at step 6500: 0.000582 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 6600: 0.000557 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 6700: 0.000579 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 6800: 0.000609 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 6900: 0.000560 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.08\n",
      "Average loss at step 7000: 0.000567 learning rate: 0.250000\n",
      "Minibatch perplexity: 0.08\n",
      "================================================================================\n",
      "Original sentence:   elimination of auth\n",
      "Predicted reverse:   nwisannlnle sl nnll\n",
      "Expected reverse :   noitanimile fo htua\n",
      "\n",
      "Original sentence:  oritarian institutio\n",
      "Predicted reverse:  natsasiln lnnlnnnnnn\n",
      "Expected reverse :  nairatiro oitutitsni\n",
      "\n",
      "Original sentence:  ns particularly the \n",
      "Predicted reverse:  si elnllnnnnnln enn \n",
      "Expected reverse :  sn ylralucitrap eht \n",
      "\n",
      "Original sentence:  state the word anarc\n",
      "Predicted reverse:  eiats snn snll snlnl\n",
      "Expected reverse :  etats eht drow crana\n",
      "\n",
      "Original sentence:  hy as most anarchist\n",
      "Predicted reverse:  yh xjjjjjjjjjjjjjjjj\n",
      "Expected reverse :  yh sa tsom tsihcrana\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# run the session\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "seq_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, seq_length)\n",
    "valid_batches = BatchGenerator(valid_text, 1, seq_length)\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    ans = 0.0\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        prediction[prediction < 1e-10] = 1e-10\n",
    "    #print(predictions.shape, labels.shape)\n",
    "    ans += np.sum(np.multiply(label, -np.log(prediction))) / label.shape[0]\n",
    "    return ans\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        reversed_batches = reverse_batches(batches)\n",
    "        feed_dict = dict()\n",
    "        for i in range(seq_length):\n",
    "            feed_dict[train_inputs[i]] = batches[i]\n",
    "            feed_dict[train_labels[i]] = reversed_batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_predictions, learning_rate],\n",
    "                                        feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f'\n",
    "                  % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, reversed_batches))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    sample = valid_batches.next()\n",
    "                    sentence = batches2strings(sample)[0]\n",
    "                    reversed_sample = reverse_batches(sample)\n",
    "                    reversed_sentence = batches2strings(reversed_sample)[0]\n",
    "                    feed_dict = dict()\n",
    "                    for i in range(seq_length):\n",
    "                        feed_dict[sample_inputs[i]] = sample[i]\n",
    "                        feed_dict[sample_labels[i]] = reversed_sample[i]\n",
    "                \n",
    "                    sample_predictions = session.run([sample_outputs], feed_dict=feed_dict)[0]\n",
    "            \n",
    "                    predicted_sentence = batches2strings(sample_predictions)[0]\n",
    "                    print('Original sentence: ', sentence)\n",
    "                    print('Predicted reverse: ', predicted_sentence)\n",
    "                    print('Expected reverse : ', reversed_sentence)\n",
    "                    print('')\n",
    "                print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf seq2seq model\n",
    "The hand-crafted model is not working so well. Now let's try to play with tf provided seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "\n",
    "filename = 'text8.zip'\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 30000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r broad generalizations and sh', 'lso supported the creation of ', 'rtoon network adult swim and c', 'ogical dispute what differs be', 'one nine seven five the rocky ', 'eady secretly obtained permiss', 'six six five eight one chee on', 'actice as old as the languages', 'r in some cases monstrous ther', 'predictions of general relativ', ' the prime minister or cabinet', 'ces of human life in present d', 'ine six two sam phillips ameri', ' to block kazaa lite from the ', ' fugitive lady one nine three ', 'f the mp three format although']\n",
      "['r daorb snoitazilareneg dna hs', 'osl detroppus eht noitaerc fo ', 'nootr krowten tluda miws dna c', 'lacigo etupsid tahw sreffid eb', 'eno enin neves evif eht ykcor ', 'ydae ylterces deniatbo ssimrep', 'xis xis evif thgie eno eehc no', 'ecitca sa dlo sa eht segaugnal', 'r ni emos sesac suortsnom reht', 'snoitciderp fo lareneg vitaler', ' eht emirp retsinim ro tenibac', 'sec fo namuh efil ni tneserp d', 'eni xis owt mas spillihp irema', ' ot kcolb aazak etil morf eht ', ' evitiguf ydal eno enin eerht ', 'f eht pm eerht tamrof hguohtla']\n",
      "['uns organizational tendencies ', 'a network of state run lodges ', 'ompanion channel boomerang tnt', 'tween interpretations is the i', 'horror picture show is the bes', 'ion to build a one eight five ', 'e zero four three d six six si', ' themselves in his play assemb', 'e are also body makeup artist ', 'ity these include studies of b', ' this produces such terms as h', 'ay india the first known perma', 'can singer one nine six eight ', 'network in november two zero z', 'four behind the evidence one n', ' there has been a significant ']\n",
      "['r daorb snoitazilareneg dna hs', 'osl detroppus eht noitaerc fo ', 'nootr krowten tluda miws dna c', 'lacigo etupsid tahw sreffid eb', 'eno enin neves evif eht ykcor ', 'ydae ylterces deniatbo ssimrep', 'xis xis evif thgie eno eehc no', 'ecitca sa dlo sa eht segaugnal', 'r ni emos sesac suortsnom reht', 'snoitciderp fo lareneg vitaler', ' eht emirp retsinim ro tenibac', 'sec fo namuh efil ni tneserp d', 'eni xis owt mas spillihp irema', ' ot kcolb aazak etil morf eht ', ' evitiguf ydal eno enin eerht ', 'f eht pm eerht tamrof hguohtla']\n",
      "['anarchism originated as a term']\n",
      "['msihcrana detanigiro sa a mret']\n",
      "[' of abuse first used against e']\n",
      "[' fo esuba tsrif desu tsniaga e']\n"
     ]
    }
   ],
   "source": [
    "# prep data\n",
    "# generate batch\n",
    "batch_size=16\n",
    "seq_length=30\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, seq_length):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._seq_length = seq_length\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, ), dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            #batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            batch[b, ] = char2id(self._text[self._cursor[b]])\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        for step in range(self._seq_length):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2strings(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * len(batches[0])\n",
    "    for b in batches:\n",
    "        s = [''.join((x[0],id2char(x[1]))) for x in zip(s, b)]\n",
    "    return s\n",
    "\n",
    "def strings2batches(strings):\n",
    "    \"\"\"convert string to 1-hot encoded batches\"\"\"\n",
    "    batches = []\n",
    "    for i in xrange(len(strings[0])):\n",
    "        batch = np.zeros(shape=(len(strings), ), dtype=np.int)\n",
    "        for j in xrange(len(strings)):\n",
    "            batch[j, ] = char2id(strings[j][i])\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "def reverse_batches(batches):\n",
    "    strings = batches2strings(batches)\n",
    "    reversed_strings = []\n",
    "    for s in strings:\n",
    "        splitted_s = s.strip().split()\n",
    "        reversed_splitted_s = [x[::-1] for x in splitted_s]\n",
    "        reversed_s = ' '.join(reversed_splitted_s)\n",
    "        if s[0] == ' ':\n",
    "            reversed_s = ' ' + reversed_s\n",
    "        if s[-1] == ' ':\n",
    "            reversed_s = reversed_s + ' '\n",
    "        reversed_strings.append(reversed_s)\n",
    "    reversed_batches = strings2batches(reversed_strings)\n",
    "    return reversed_batches\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, seq_length)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 30)\n",
    "\n",
    "batches1 = train_batches.next()\n",
    "# print(batches1)\n",
    "labels1 = reverse_batches(batches1)\n",
    "print(batches2strings(batches1))\n",
    "print(batches2strings(labels1))\n",
    "\n",
    "batches2 = train_batches.next()\n",
    "labels2 = reverse_batches(batches1)\n",
    "print(batches2strings(batches2))\n",
    "print(batches2strings(labels2))\n",
    "\n",
    "valids1 = valid_batches.next()\n",
    "valid_labels1 = reverse_batches(valids1)\n",
    "print(batches2strings(valids1))\n",
    "print(batches2strings(valid_labels1))\n",
    "\n",
    "valids2 = valid_batches.next()\n",
    "valid_labels2 = reverse_batches(valids2)\n",
    "print(batches2strings(valids2))\n",
    "print(batches2strings(valid_labels2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train and run the multi-layer seq2seq model\n",
    "cell_size = 128\n",
    "seq_length = 20\n",
    "batch_size = 64\n",
    "vocab_size = 27 # [a-z] + space\n",
    "embedding_size = 27\n",
    "\n",
    "from tensorflow.contrib.legacy_seq2seq import embedding_rnn_seq2seq, sequence_loss\n",
    "\n",
    "# build graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs = [tf.placeholder(tf.int32, shape = (None,))\n",
    "                      for i in range(seq_length)]\n",
    "    \n",
    "    train_labels = [tf.placeholder(tf.int32, shape = (None,))\n",
    "              for i in range(seq_length)]\n",
    "    \n",
    "    train_decoder_input = [tf.zeros(shape = (batch_size,), dtype = tf.int32)] + train_labels[:-1]\n",
    "    \n",
    "    cell = tf.contrib.rnn.GRUCell(cell_size)\n",
    "    \n",
    "    with tf.variable_scope('seq2seq') as scope:\n",
    "        train_outputs, _ = embedding_rnn_seq2seq(train_inputs, train_decoder_input,\n",
    "                                            cell, vocab_size, vocab_size, embedding_size,\n",
    "                                           feed_previous = False)\n",
    "    \n",
    "    loss_weights = [tf.ones(shape = (batch_size,), dtype = tf.float32) for i in range(seq_length)]\n",
    "    loss = sequence_loss(train_outputs, train_labels, loss_weights)\n",
    "    \n",
    "    # optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        1.0, global_step, 2000, 0.5, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.5)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # sample verification\n",
    "    sample_inputs = [tf.placeholder(tf.int32, shape = (1, ))\n",
    "                      for i in range(seq_length)]\n",
    "    sample_decoder_input = [tf.zeros(shape = (1,), dtype = tf.int32) for i in range(seq_length)]\n",
    "    \n",
    "    with tf.variable_scope('seq2seq', reuse = True) as scope:\n",
    "        sample_outputs, _ = embedding_rnn_seq2seq(sample_inputs, sample_decoder_input, \n",
    "                                           cell, vocab_size, vocab_size, embedding_size,\n",
    "                                           feed_previous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.288364 learning rate: 1.000000\n",
      "Minibatch perplexity: 235.84\n",
      "================================================================================\n",
      "Original sentence:  anarchism originated\n",
      "Predicted reverse:                      \n",
      "Expected reverse :  msihcrana detanigiro\n",
      "\n",
      "Original sentence:   as a term of abuse \n",
      "Predicted reverse:                      \n",
      "Expected reverse :   sa a mret fo esuba \n",
      "\n",
      "Original sentence:  first used against e\n",
      "Predicted reverse:                      \n",
      "Expected reverse :  tsrif desu tsniaga e\n",
      "\n",
      "Original sentence:  arly working class r\n",
      "Predicted reverse:                      \n",
      "Expected reverse :  ylra gnikrow ssalc r\n",
      "\n",
      "Original sentence:  adicals including th\n",
      "Predicted reverse:                      \n",
      "Expected reverse :  slacida gnidulcni ht\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 100: 2.787652 learning rate: 1.000000\n",
      "Minibatch perplexity: 65.28\n",
      "Average loss at step 200: 2.499255 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.06\n",
      "Average loss at step 300: 2.263642 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.07\n",
      "Average loss at step 400: 2.118869 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Average loss at step 500: 1.992718 learning rate: 1.000000\n",
      "Minibatch perplexity: -1.52\n",
      "Average loss at step 600: 1.882803 learning rate: 1.000000\n",
      "Minibatch perplexity: -4.04\n",
      "Average loss at step 700: 1.779458 learning rate: 1.000000\n",
      "Minibatch perplexity: -9.16\n",
      "Average loss at step 800: 1.701805 learning rate: 1.000000\n",
      "Minibatch perplexity: -11.06\n",
      "Average loss at step 900: 1.610935 learning rate: 1.000000\n",
      "Minibatch perplexity: -9.64\n",
      "Average loss at step 1000: 1.532242 learning rate: 1.000000\n",
      "Minibatch perplexity: -12.31\n",
      "================================================================================\n",
      "Original sentence:  e diggers of the eng\n",
      "Predicted reverse:  ered senim eno eht e\n",
      "Expected reverse :  e sreggid fo eht gne\n",
      "\n",
      "Original sentence:  lish revolution and \n",
      "Predicted reverse:  noitilpoc niderpa na\n",
      "Expected reverse :  hsil noitulover dna \n",
      "\n",
      "Original sentence:  the sans culottes of\n",
      "Predicted reverse:  eht setsanits fo owt\n",
      "Expected reverse :  eht snas settoluc fo\n",
      "\n",
      "Original sentence:   the french revoluti\n",
      "Predicted reverse:  eht ereht niromerht \n",
      "Expected reverse :   eht hcnerf itulover\n",
      "\n",
      "Original sentence:  on whilst the term i\n",
      "Predicted reverse:  noit eht serht lairh\n",
      "Expected reverse :  no tslihw eht mret i\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 1100: 1.491646 learning rate: 1.000000\n",
      "Minibatch perplexity: -19.56\n",
      "Average loss at step 1200: 1.430931 learning rate: 1.000000\n",
      "Minibatch perplexity: -18.46\n",
      "Average loss at step 1300: 1.365731 learning rate: 1.000000\n",
      "Minibatch perplexity: -24.87\n",
      "Average loss at step 1400: 1.300531 learning rate: 1.000000\n",
      "Minibatch perplexity: -23.41\n",
      "Average loss at step 1500: 1.256366 learning rate: 1.000000\n",
      "Minibatch perplexity: -20.24\n",
      "Average loss at step 1600: 1.211251 learning rate: 1.000000\n",
      "Minibatch perplexity: -25.11\n",
      "Average loss at step 1700: 1.168468 learning rate: 1.000000\n",
      "Minibatch perplexity: -28.58\n",
      "Average loss at step 1800: 1.136420 learning rate: 1.000000\n",
      "Minibatch perplexity: -26.95\n",
      "Average loss at step 1900: 1.069872 learning rate: 1.000000\n",
      "Minibatch perplexity: -31.02\n",
      "Average loss at step 2000: 1.049838 learning rate: 0.500000\n",
      "Minibatch perplexity: -30.02\n",
      "================================================================================\n",
      "Original sentence:  s still used in a pe\n",
      "Predicted reverse:  s slitid eht na es c\n",
      "Expected reverse :  s llits desu ni a ep\n",
      "\n",
      "Original sentence:  jorative way to desc\n",
      "Predicted reverse:  evitarot ya owt dese\n",
      "Expected reverse :  evitaroj yaw ot csed\n",
      "\n",
      "Original sentence:  ribe any act that us\n",
      "Predicted reverse:  ereb yani taht eb tc\n",
      "Expected reverse :  ebir yna tca taht su\n",
      "\n",
      "Original sentence:  ed violent means to \n",
      "Predicted reverse:  de tnevolsi no etam \n",
      "Expected reverse :  de tneloiv snaem ot \n",
      "\n",
      "Original sentence:  destroy the organiza\n",
      "Predicted reverse:  setiroh era naigaroh\n",
      "Expected reverse :  yortsed eht azinagro\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 2100: 0.839185 learning rate: 0.500000\n",
      "Minibatch perplexity: -31.77\n",
      "Average loss at step 2200: 0.822304 learning rate: 0.500000\n",
      "Minibatch perplexity: -33.47\n",
      "Average loss at step 2300: 0.821295 learning rate: 0.500000\n",
      "Minibatch perplexity: -33.80\n",
      "Average loss at step 2400: 0.793355 learning rate: 0.500000\n",
      "Minibatch perplexity: -33.44\n",
      "Average loss at step 2500: 0.787235 learning rate: 0.500000\n",
      "Minibatch perplexity: -35.84\n",
      "Average loss at step 2600: 0.777887 learning rate: 0.500000\n",
      "Minibatch perplexity: -34.03\n",
      "Average loss at step 2700: 0.780521 learning rate: 0.500000\n",
      "Minibatch perplexity: -32.63\n",
      "Average loss at step 2800: 0.778958 learning rate: 0.500000\n",
      "Minibatch perplexity: -34.61\n",
      "Average loss at step 2900: 0.734359 learning rate: 0.500000\n",
      "Minibatch perplexity: -35.44\n",
      "Average loss at step 3000: 0.720437 learning rate: 0.500000\n",
      "Minibatch perplexity: -35.10\n",
      "================================================================================\n",
      "Original sentence:  tion of society it h\n",
      "Predicted reverse:  noit fo tseici yht i\n",
      "Expected reverse :  noit fo yteicos ti h\n",
      "\n",
      "Original sentence:  as also been taken u\n",
      "Predicted reverse:  sa osla eneved utan \n",
      "Expected reverse :  sa osla neeb nekat u\n",
      "\n",
      "Original sentence:  p as a positive labe\n",
      "Predicted reverse:  s a tseb evilib esib\n",
      "Expected reverse :  p sa a evitisop ebal\n",
      "\n",
      "Original sentence:  l by self defined an\n",
      "Predicted reverse:  l ylb fes devinem na\n",
      "Expected reverse :  l yb fles denifed na\n",
      "\n",
      "Original sentence:  archists the word an\n",
      "Predicted reverse:  stsihcre ta dehw ron\n",
      "Expected reverse :  stsihcra eht drow na\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 3100: 0.713022 learning rate: 0.500000\n",
      "Minibatch perplexity: -37.62\n",
      "Average loss at step 3200: 0.712603 learning rate: 0.500000\n",
      "Minibatch perplexity: -35.67\n",
      "Average loss at step 3300: 0.678408 learning rate: 0.500000\n",
      "Minibatch perplexity: -37.25\n",
      "Average loss at step 3400: 0.679771 learning rate: 0.500000\n",
      "Minibatch perplexity: -37.51\n",
      "Average loss at step 3500: 0.684987 learning rate: 0.500000\n",
      "Minibatch perplexity: -36.96\n",
      "Average loss at step 3600: 0.673886 learning rate: 0.500000\n",
      "Minibatch perplexity: -38.52\n",
      "Average loss at step 3700: 0.628132 learning rate: 0.500000\n",
      "Minibatch perplexity: -38.59\n",
      "Average loss at step 3800: 0.639564 learning rate: 0.500000\n",
      "Minibatch perplexity: -38.49\n",
      "Average loss at step 3900: 0.623032 learning rate: 0.500000\n",
      "Minibatch perplexity: -40.28\n",
      "Average loss at step 4000: 0.599520 learning rate: 0.250000\n",
      "Minibatch perplexity: -38.90\n",
      "================================================================================\n",
      "Original sentence:  archism is derived f\n",
      "Predicted reverse:  smihcar si devired f\n",
      "Expected reverse :  msihcra si devired f\n",
      "\n",
      "Original sentence:  rom the greek withou\n",
      "Predicted reverse:  mor eht ekrow eutihw\n",
      "Expected reverse :  mor eht keerg uohtiw\n",
      "\n",
      "Original sentence:  t archons ruler chie\n",
      "Predicted reverse:  tn srohcna reveb hir\n",
      "Expected reverse :  t snohcra relur eihc\n",
      "\n",
      "Original sentence:  f king anarchism as \n",
      "Predicted reverse:  f gnif sdnairah mac \n",
      "Expected reverse :  f gnik msihcrana sa \n",
      "\n",
      "Original sentence:  a political philosop\n",
      "Predicted reverse:  a laicitop solpohilp\n",
      "Expected reverse :  a lacitilop posolihp\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 4100: 0.493380 learning rate: 0.250000\n",
      "Minibatch perplexity: -39.35\n",
      "Average loss at step 4200: 0.466461 learning rate: 0.250000\n",
      "Minibatch perplexity: -39.86\n",
      "Average loss at step 4300: 0.462833 learning rate: 0.250000\n",
      "Minibatch perplexity: -39.45\n",
      "Average loss at step 4400: 0.466290 learning rate: 0.250000\n",
      "Minibatch perplexity: -40.72\n",
      "Average loss at step 4500: 0.463435 learning rate: 0.250000\n",
      "Minibatch perplexity: -41.23\n",
      "Average loss at step 4600: 0.460218 learning rate: 0.250000\n",
      "Minibatch perplexity: -40.86\n",
      "Average loss at step 4700: 0.461886 learning rate: 0.250000\n",
      "Minibatch perplexity: -41.44\n",
      "Average loss at step 4800: 0.467003 learning rate: 0.250000\n",
      "Minibatch perplexity: -42.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4900: 0.448783 learning rate: 0.250000\n",
      "Minibatch perplexity: -41.64\n",
      "Average loss at step 5000: 0.453927 learning rate: 0.250000\n",
      "Minibatch perplexity: -43.05\n",
      "================================================================================\n",
      "Original sentence:  hy is the belief tha\n",
      "Predicted reverse:  yh si eht feileb aht\n",
      "Expected reverse :  yh si eht feileb aht\n",
      "\n",
      "Original sentence:  t rulers are unneces\n",
      "Predicted reverse:  t srelur es ebneveg \n",
      "Expected reverse :  t srelur era secennu\n",
      "\n",
      "Original sentence:  sary and should be a\n",
      "Predicted reverse:  yrad ssaw dnuolb eht\n",
      "Expected reverse :  yras dna dluohs eb a\n",
      "\n",
      "Original sentence:  bolished although th\n",
      "Predicted reverse:  desihwolb hcohtula h\n",
      "Expected reverse :  dehsilob hguohtla ht\n",
      "\n",
      "Original sentence:  ere are differing in\n",
      "Predicted reverse:  ere rega emirfnid ni\n",
      "Expected reverse :  ere era gnireffid ni\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 5100: 0.453043 learning rate: 0.250000\n",
      "Minibatch perplexity: -40.44\n",
      "Average loss at step 5200: 0.445232 learning rate: 0.250000\n",
      "Minibatch perplexity: -41.39\n",
      "Average loss at step 5300: 0.444273 learning rate: 0.250000\n",
      "Minibatch perplexity: -40.37\n",
      "Average loss at step 5400: 0.435304 learning rate: 0.250000\n",
      "Minibatch perplexity: -41.83\n",
      "Average loss at step 5500: 0.431138 learning rate: 0.250000\n",
      "Minibatch perplexity: -41.83\n",
      "Average loss at step 5600: 0.430444 learning rate: 0.250000\n",
      "Minibatch perplexity: -42.35\n",
      "Average loss at step 5700: 0.441263 learning rate: 0.250000\n",
      "Minibatch perplexity: -42.05\n",
      "Average loss at step 5800: 0.422173 learning rate: 0.250000\n",
      "Minibatch perplexity: -42.60\n",
      "Average loss at step 5900: 0.422903 learning rate: 0.250000\n",
      "Minibatch perplexity: -43.04\n",
      "Average loss at step 6000: 0.419427 learning rate: 0.125000\n",
      "Minibatch perplexity: -43.29\n",
      "================================================================================\n",
      "Original sentence:  terpretations of wha\n",
      "Predicted reverse:  stnoitarerep fo haw \n",
      "Expected reverse :  snoitaterpret fo ahw\n",
      "\n",
      "Original sentence:  t this means anarchi\n",
      "Predicted reverse:  t siht snaem nihcara\n",
      "Expected reverse :  t siht snaem ihcrana\n",
      "\n",
      "Original sentence:  sm also refers to re\n",
      "Predicted reverse:  mos sla resefo er t \n",
      "Expected reverse :  ms osla srefer ot er\n",
      "\n",
      "Original sentence:  lated social movemen\n",
      "Predicted reverse:  detal laicom senevmo\n",
      "Expected reverse :  detal laicos nemevom\n",
      "\n",
      "Original sentence:  ts that advocate the\n",
      "Predicted reverse:  ts taht ecatodle eht\n",
      "Expected reverse :  st taht etacovda eht\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 6100: 0.349320 learning rate: 0.125000\n",
      "Minibatch perplexity: -42.95\n",
      "Average loss at step 6200: 0.335326 learning rate: 0.125000\n",
      "Minibatch perplexity: -42.68\n",
      "Average loss at step 6300: 0.333414 learning rate: 0.125000\n",
      "Minibatch perplexity: -43.28\n",
      "Average loss at step 6400: 0.329761 learning rate: 0.125000\n",
      "Minibatch perplexity: -41.77\n",
      "Average loss at step 6500: 0.321986 learning rate: 0.125000\n",
      "Minibatch perplexity: -43.71\n",
      "Average loss at step 6600: 0.319335 learning rate: 0.125000\n",
      "Minibatch perplexity: -43.34\n",
      "Average loss at step 6700: 0.318133 learning rate: 0.125000\n",
      "Minibatch perplexity: -43.88\n",
      "Average loss at step 6800: 0.323140 learning rate: 0.125000\n",
      "Minibatch perplexity: -43.43\n",
      "Average loss at step 6900: 0.321322 learning rate: 0.125000\n",
      "Minibatch perplexity: -43.80\n",
      "Average loss at step 7000: 0.324124 learning rate: 0.125000\n",
      "Minibatch perplexity: -43.24\n",
      "================================================================================\n",
      "Original sentence:   elimination of auth\n",
      "Predicted reverse:   noitaimnile fo htua\n",
      "Expected reverse :   noitanimile fo htua\n",
      "\n",
      "Original sentence:  oritarian institutio\n",
      "Predicted reverse:  nairatro itiutosnit \n",
      "Expected reverse :  nairatiro oitutitsni\n",
      "\n",
      "Original sentence:  ns particularly the \n",
      "Predicted reverse:  sn yllaruricap teht \n",
      "Expected reverse :  sn ylralucitrap eht \n",
      "\n",
      "Original sentence:  state the word anarc\n",
      "Predicted reverse:  etats eht rod gnarb \n",
      "Expected reverse :  etats eht drow crana\n",
      "\n",
      "Original sentence:  hy as most anarchist\n",
      "Predicted reverse:  yh sa tsom sitrahcna\n",
      "Expected reverse :  yh sa tsom tsihcrana\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# run the session\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "seq_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, seq_length)\n",
    "valid_batches = BatchGenerator(valid_text, 1, seq_length)\n",
    "\n",
    "def logprob(predictions, targets):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    ans = 0.0\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        prediction[prediction < 1e-10] = 1e-10\n",
    "        label = np.zeros(shape=prediction.shape, dtype=np.float)\n",
    "        for j in xrange(prediction.shape[0]):\n",
    "            label[j, target[j]] = 1.0\n",
    "        # print(prediction.shape, label)\n",
    "        ans += np.sum(np.multiply(label, -np.log(prediction))) / label.shape[0]\n",
    "    return ans\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        reversed_batches = reverse_batches(batches)\n",
    "        feed_dict = dict()\n",
    "        for i in range(seq_length):\n",
    "            feed_dict[train_inputs[i]] = batches[i]\n",
    "            feed_dict[train_labels[i]] = reversed_batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_outputs, learning_rate],\n",
    "                                        feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f'\n",
    "                  % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch log perplexity: %.2f' % float(\n",
    "                (logprob(predictions, reversed_batches))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    sample = valid_batches.next()\n",
    "                    sentence = batches2strings(sample)[0]\n",
    "                    reversed_sample = reverse_batches(sample)\n",
    "                    reversed_sentence = batches2strings(reversed_sample)[0]\n",
    "                    feed_dict = dict()\n",
    "                    for i in range(seq_length):\n",
    "                        feed_dict[sample_inputs[i]] = sample[i]\n",
    "                \n",
    "                    sample_predictions = session.run([sample_outputs], feed_dict=feed_dict)[0]\n",
    "                    \n",
    "                    sample_prediction_targets = [np.argmax(sample_prediction, 1)\n",
    "                                                for sample_prediction in sample_predictions]\n",
    "                    predicted_sentence = batches2strings(sample_prediction_targets)[0]\n",
    "                    print('Original sentence: ', sentence)\n",
    "                    print('Predicted reverse: ', predicted_sentence)\n",
    "                    print('Expected reverse : ', reversed_sentence)\n",
    "                    print('')\n",
    "                print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "Very interesting experiment and very exciting results even with some prediction error! Maybe the extra layer of embedding helped. I believe there is still a large room for improvement by fine tuning the model parameter. For example, increasing the number of dimensions in each cell should be able to give it a larger \"memory\" to do better for long words. The hand-crafted model should also be able to improve with more layers and larger dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
